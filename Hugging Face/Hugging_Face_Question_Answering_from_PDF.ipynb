{"cells":[{"cell_type":"markdown","id":"bedfa998","metadata":{"id":"bedfa998","papermill":{},"tags":[]},"source":[]},{"cell_type":"markdown","id":"dfa971dd","metadata":{"id":"dfa971dd","papermill":{},"tags":[]},"source":["# Hugging Face - Question Answering from PDF\n"]},{"cell_type":"markdown","id":"9776f972","metadata":{"id":"9776f972","papermill":{},"tags":[]},"source":[]},{"cell_type":"markdown","id":"36ddb4e8","metadata":{"id":"36ddb4e8","papermill":{},"tags":[]},"source":["**Author:** [Muhammad Talha Khan](https://www.linkedin.com/in/muhtalhakhan/)"]},{"cell_type":"markdown","id":"121e3f58","metadata":{"id":"121e3f58","papermill":{},"tags":[]},"source":["**Description**: This Transformers QA Pipeline shows a Question Answering models that can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. This specific example is set with the Bitcoin white paper but you can put any PDF.\n","\n","The PDF will server as context which will used for questions answering."]},{"cell_type":"markdown","id":"df7f0816","metadata":{"id":"df7f0816","papermill":{},"tags":[]},"source":["## Input"]},{"cell_type":"markdown","id":"e4b53a36","metadata":{"execution":{"iopub.execute_input":"2022-11-02T22:06:30.070094Z","iopub.status.busy":"2022-11-02T22:06:30.069794Z","iopub.status.idle":"2022-11-02T22:06:30.072975Z","shell.execute_reply":"2022-11-02T22:06:30.072331Z","shell.execute_reply.started":"2022-11-02T22:06:30.070062Z"},"papermill":{},"tags":[],"id":"e4b53a36"},"source":["### Install Packages"]},{"cell_type":"code","execution_count":1,"id":"72d1eac5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-11-02T22:06:46.895803Z","iopub.status.busy":"2022-11-02T22:06:46.895542Z","iopub.status.idle":"2022-11-02T22:06:56.209620Z","shell.execute_reply":"2022-11-02T22:06:56.208914Z","shell.execute_reply.started":"2022-11-02T22:06:46.895778Z"},"id":"72d1eac5","outputId":"cf30d77f-3b3c-4c3b-83fe-65a7e15c3b16","papermill":{},"tags":[],"executionInfo":{"status":"ok","timestamp":1673638330935,"user_tz":0,"elapsed":6484,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":2,"id":"1ea37c89","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-11-02T21:56:25.255594Z","iopub.status.busy":"2022-11-02T21:56:25.255416Z","iopub.status.idle":"2022-11-02T21:56:30.480930Z","shell.execute_reply":"2022-11-02T21:56:30.480200Z","shell.execute_reply.started":"2022-11-02T21:56:25.255573Z"},"id":"1ea37c89","outputId":"b1534109-faa0-44fa-af0f-42a6377ad95f","papermill":{},"tags":[],"executionInfo":{"status":"ok","timestamp":1673638348656,"user_tz":0,"elapsed":13888,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers"]},{"cell_type":"markdown","id":"bbc4c01a","metadata":{"papermill":{},"tags":[],"id":"bbc4c01a"},"source":["Use \"--user\" if it asks for permission prompt."]},{"cell_type":"code","execution_count":3,"id":"7e514057","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:30.482590Z","iopub.status.busy":"2022-11-02T21:56:30.482367Z","iopub.status.idle":"2022-11-02T21:56:36.065302Z","shell.execute_reply":"2022-11-02T21:56:36.064638Z","shell.execute_reply.started":"2022-11-02T21:56:30.482560Z"},"papermill":{},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7e514057","executionInfo":{"status":"ok","timestamp":1673638368104,"user_tz":0,"elapsed":9459,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}},"outputId":"097f2d92-b4e1-4a7f-ab0e-c7c32aca6b30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.4.0)\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}],"source":["!pip install PyPDF2"]},{"cell_type":"code","execution_count":4,"id":"c5ea0590","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:36.066692Z","iopub.status.busy":"2022-11-02T21:56:36.066462Z","iopub.status.idle":"2022-11-02T21:56:41.219375Z","shell.execute_reply":"2022-11-02T21:56:41.218679Z","shell.execute_reply.started":"2022-11-02T21:56:36.066662Z"},"papermill":{},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"c5ea0590","executionInfo":{"status":"ok","timestamp":1673638374696,"user_tz":0,"elapsed":6600,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}},"outputId":"f88d2e50-cdeb-40f4-b1b2-2fbfe17388b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (1.24.3)\n"]}],"source":["!pip install urllib3"]},{"cell_type":"markdown","id":"e3f7133c","metadata":{"id":"e3f7133c","papermill":{},"tags":[]},"source":["### Import Libraries\n"]},{"cell_type":"code","execution_count":5,"id":"99f89fd4","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:41.220795Z","iopub.status.busy":"2022-11-02T21:56:41.220578Z","iopub.status.idle":"2022-11-02T21:56:48.720459Z","shell.execute_reply":"2022-11-02T21:56:48.719822Z","shell.execute_reply.started":"2022-11-02T21:56:41.220766Z"},"papermill":{},"tags":[],"id":"99f89fd4","executionInfo":{"status":"ok","timestamp":1673638387212,"user_tz":0,"elapsed":11790,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}}},"outputs":[],"source":["from transformers import pipeline\n","import urllib.request\n","import PyPDF2\n","import io"]},{"cell_type":"markdown","id":"63d3834b","metadata":{"papermill":{},"tags":[],"id":"63d3834b"},"source":["### Add the Document Path"]},{"cell_type":"code","execution_count":7,"id":"5d02938f","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:48.721667Z","iopub.status.busy":"2022-11-02T21:56:48.721432Z","iopub.status.idle":"2022-11-02T21:56:49.846905Z","shell.execute_reply":"2022-11-02T21:56:49.846220Z","shell.execute_reply.started":"2022-11-02T21:56:48.721638Z"},"papermill":{},"tags":[],"id":"5d02938f","executionInfo":{"status":"ok","timestamp":1673638522773,"user_tz":0,"elapsed":1371,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}}},"outputs":[],"source":["URL = 'https://arxiv.org/pdf/2301.02593.pdf'\n","req = urllib.request.Request(URL, headers={'User-Agent' : \"Chrome\"})\n","remote_file = urllib.request.urlopen(req).read()\n","remote_file_bytes = io.BytesIO(remote_file)\n","pdfdoc_remote = PyPDF2.PdfReader(remote_file_bytes)"]},{"cell_type":"markdown","id":"c722046c","metadata":{"papermill":{},"tags":[],"id":"c722046c"},"source":["You can change the URL path to the desired one relating to any of the PDF."]},{"cell_type":"markdown","id":"ec280229","metadata":{"papermill":{},"tags":[],"id":"ec280229"},"source":["## Model"]},{"cell_type":"markdown","id":"38e868fc","metadata":{"papermill":{},"tags":[],"id":"38e868fc"},"source":["### Read Text from File"]},{"cell_type":"code","execution_count":11,"id":"37c02550","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:49.851998Z","iopub.status.busy":"2022-11-02T21:56:49.850386Z","iopub.status.idle":"2022-11-02T21:56:50.414792Z","shell.execute_reply":"2022-11-02T21:56:50.414140Z","shell.execute_reply.started":"2022-11-02T21:56:49.851963Z"},"papermill":{},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"37c02550","executionInfo":{"status":"ok","timestamp":1673638628702,"user_tz":0,"elapsed":1805,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}},"outputId":"00e83a3b-1d15-40eb-9fd2-7e34c48f097a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n"]}],"source":["pdf_text = \"\"    \n","    \n","for i in range(len(pdfdoc_remote.pages)):\n","    print(i)\n","    page = pdfdoc_remote.pages[i]\n","    page_content = page.extract_text()\n","    pdf_text += page_content    "]},{"cell_type":"markdown","id":"59d1c654","metadata":{"papermill":{},"tags":[],"id":"59d1c654"},"source":["### Generate the text data from the pdf file "]},{"cell_type":"code","execution_count":12,"id":"d2dc6752","metadata":{"execution":{"iopub.execute_input":"2022-11-02T21:56:50.417044Z","iopub.status.busy":"2022-11-02T21:56:50.416681Z","iopub.status.idle":"2022-11-02T21:56:50.421344Z","shell.execute_reply":"2022-11-02T21:56:50.420755Z","shell.execute_reply.started":"2022-11-02T21:56:50.417012Z"},"papermill":{},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"d2dc6752","executionInfo":{"status":"ok","timestamp":1673638633902,"user_tz":0,"elapsed":264,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}},"outputId":"f3c5a28c-91fb-4c33-a9fa-09528a6a98b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Multi-Agent Reinforcement Learning for Fast-Timescale\n","Demand Response of Residential Loads\n","Vincent Mai\n","Robotics & Embodied AI Lab, Mila\n","Université de Montréal, Canada\n","vincent.mai@umontreal.caPhilippe Maisonneuve\n","GERAD & Mila\n","Polytechnique Montréal, Canada\n","philippe.maisonneuve@polymtl.caTianyu Zhang\n","Mila\n","Université de Montréal, Canada\n","tianyu.zhang@mila.quebec\n","Hadi Nekoei\n","Mila\n","Université de Montréal, Canada\n","nekoeihe@mila.quebecLiam Paull\n","Robotics & Embodied AI Lab, Mila\n","Université de Montréal, Canada\n","liam.paull@umontreal.caAntoine Lesage-Landry\n","GERAD & Mila\n","Polytechnique Montréal, Canada\n","antoine.lesage-landry@polymtl.ca\n","ABSTRACT\n","To integrate high amounts of renewable energy resources, electrical\n","power grids must be able to cope with high amplitude, fast timescale\n","variations in power generation. Frequency regulation through de-\n","mand response has the potential to coordinate temporally flexible\n","loads, such as air conditioners, to counteract these variations. Ex-\n","isting approaches for discrete control with dynamic constraints\n","struggle to provide satisfactory performance for fast timescale ac-\n","tion selection with hundreds of agents. We propose a decentralized\n","agent trained with multi-agent proximal policy optimization with\n","localized communication. We explore two communication frame-\n","works: hand-engineered, or learned through targeted multi-agent\n","communication. The resulting policies perform well and robustly\n","for frequency regulation, and scale seamlessly to arbitrary numbers\n","of houses for constant processing times.\n","KEYWORDS\n","Multi-agent reinforcement learning, Demand response, Power sys-\n","tems, Renewable integration, Communication, Coordination\n","1 INTRODUCTION\n","To achieve the United Nations’ climate change target of limiting\n","global warming at +1.5 °C, global electricity generation must tran-\n","sition from fossil fuels to renewable energy sources such as wind\n","turbines and solar panels. In 2019, according to the International\n","Energy Agency, electricity and heat production accounted for 40 %\n","of global emissions [ 3], as 64% of it is generated from burning fossil\n","fuel [ 1]. The electricity sector must thus move from a conventional,\n","fuel-burning paradigm to a renewable, natural phenomenon-based\n","generation, e.g., wind turbines and solar photovoltaics. Renew-\n","able energy generation is subject to short-term, high-amplitude\n","variations, referred to as intermittency. As an example, a cloud\n","passing will lead to a sudden drop in the solar-based generation,\n","followed by a sharp increase when the sky becomes clear again.\n","These changes can happen at the scale of a few seconds, and create\n","major challenges for power grid operators: to ensure the stability of\n","the electric grid, a near-perfect balance between the power demand\n","and the generation is critical [ 29]. In other words, power generation\n","and consumption must be equal at all times. Hence, trading a con-\n","stant, deterministic generation for an intermittent, uncertain oneexacerbates the need for power balancing. At the second timescale,\n","this balancing task is referred to as frequency regulation [9, 51].\n","On the power generation side, solutions such as excess energy\n","storage in batteries or support with fossil fuel plants require large\n","investments and are not renewable respectively. Alternatively, de-\n","mand response programs [ 49] can be introduced to mitigate renew-\n","able intermittency [ 51]. The demand response approach aims at\n","adjusting the power demand to meet the supply by coordinating\n","loads temporally. These loads must be flexible, i.e., capable of mod-\n","ulating their consumption while fulfilling their own purpose. This\n","does not apply to, for example, computer monitors, which must be\n","fully powered when they are in use. Thermostatic loads, such as\n","heating, air conditioning or water heaters, are instead ideal candi-\n","dates: they do not need to be turned on at all times, as long as the\n","temperature of the air/water is within the user’s preference range\n","[12]. They are also widely deployed and they represent a significant\n","part of global power consumption [ 2,40]. The frequency regulation\n","objective differs from peak-shaving, for which the objective is load\n","shifting over, e.g., a day. It is instead to leverage the loads’ flexibility\n","to balance out high-frequency variations in power generation.\n","In this paper, we focus on the task of fast timescale demand\n","response for frequency regulation using residential air condition-\n","ers. This presents several physical and algorithmic constraints: (1)\n","air conditioners are discretely powered, i.e., onoroff, which\n","limits the control flexibility; (2) they are subject to hardware dy-\n","namic constraints such as lockout: once turned off, they must\n","wait some time before being allowed to turn back onto protect the\n","compressor; (3) as the context is residential, privacy is important\n","andcommunications should be limited ; (4) to provide enough\n","power flexibility to the grid, a large aggregation of loads must be\n","considered: the method must be scalable ; (5) for easier implemen-\n","tation, the control should also be decentralized with localized\n","communications ; (6) the decisions must be taken at a few sec-\n","onds timescale ; and finally (7) the control algorithm should be\n","able to cope with uncertainty in the future regulation signal.\n","These constraints impede the deployment of classical methods.\n","Greedy algorithms are centralized and have difficulty accounting\n","for long-term dynamic constraints [ 35]. Standard model predictive\n","control is also centralized, and even decentralized versions solve\n","a multi-period combinatorial optimization problem that does not\n","scale with the number of agents [ 26?]. We propose to tackle this\n","problem by using multi-agent reinforcement learning (MARL) toarXiv:2301.02593v1  [cs.MA]  6 Jan 2023learn decentralized and scalable policies (4) with discrete and con-\n","strained control (1, 2) and limited and localized communications\n","(3, 5). Once learned, these policies can take the best decisions in\n","real time (6) based on expected value over uncertainty (7). As this\n","problem combines the most important current challenges of MARL,\n","i.e., communication, long-term credit assignment, coordination,\n","and scalability [ 21], it is also an interesting benchmark for MARL\n","algorithms. We train our agents with Multi-Agent Proximal Policy\n","Optimization (MA-PPO) [ 60] with Centralized Training, Decentral-\n","ized Execution (CTDE). [ 28]. Two local communication frameworks\n","are tested – hand-engineered and learned – and both outperform\n","the baselines. Our main contributions are threefold:\n","•an open source, multi-agent environment1simulating the\n","real-world problem of frequency regulation through demand\n","response at the second timescale. The simulator is compatible\n","with the OpenAI Gym [11] framework.\n","•two decentralized, fast-responding agents1trained by MA-\n","PPO. The first one has a hand-engineered communication\n","strategy, while the second one learns what data to share\n","through Targeted Multi-Agent Communication (TarMAC)\n","[17]. Both outperform baselines on two-day simulations.\n","•an in-depth analysis of the dynamics, communications, scal-\n","ability and robustness of the trained agents.\n","In the next section, we describe prior work in the field of demand\n","response and MARL. In Section 3, we describe the environment and\n","formulate the problem. The classical and learning-based methods\n","are described in Section 4. Finally, Section 5 presents the experi-\n","mental results and analyses of the agents’ performance, dynamics,\n","robustness, and scalability.\n","2 RELATED WORKS\n","Frequency regulation through demand response is commonly tack-\n","led by model predictive control (MPC) [ 26,33,39,40,43,56], where\n","the best action is chosen based on trajectory prediction over a given\n","horizon, sometimes combined with machine learning [ 4,19,32].\n","Apart from [37], these works do not consider short-term dynamic\n","constraints such as lockout. MPC approaches rely on mixed-integer\n","programming, which does not scale sustainably with higher num-\n","bers of agents, preventing control at fast timescales. Moreover, these\n","works generally require a centralized entity to access residences’\n","data, leading to confidentiality issues. An alternative method of\n","multipliers-based distributed MPC approach was proposed in [ 13].\n","This approach did not consider the lockout constraint and is not\n","compatible with fast timescale decision-making as it requires multi-\n","ple centralized communication rounds at each time step in addition\n","to solving several optimization problems and converting continuous\n","setpoints to binary actions.\n","To tackle these problems, online optimization (OO) approaches [ 34,\n","62] have been used because of their high computational efficiency\n","and scalability. In particular, [ 35] deploys OO for frequency regula-\n","tion with binary control settings as is the case for ACs. However,\n","these methods rely on greedy optimization and their lack of fore-\n","sight leads to limited performance when facing dynamic constraints.\n","Reinforcement learning (RL) methods have been developed to ad-\n","dress the longer timescale power balance problems such as peak\n","1The code is hosted on https://github.com/ALLabMTL/MARL_for_fast_timescale_DR.shaving through demand response [ 6] or coordination of loads and\n","generators [ 47,58]. The CityLearn environment [ 52] proposes a\n","standard environment for multi-agent RL (MARL) for demand re-\n","sponse, upon which are developed methods such as [ 45] to regulate\n","the voltage magnitude in distribution networks using smart invert-\n","ers and intelligent energy storage management, and [ 53] for load\n","shaping of grid-interactive connected buildings. The AlphaBuilding\n","ResCommunity environment [ 54] then implements detailed ther-\n","mal models. Both CityLearn and AlphaBuilding ResCommunity,\n","however, consider longer timescale control, which makes them inad-\n","equate for high-frequency regulation and removes the ACs’ lockout\n","and binary constraints. The PowerGridworld [ 10] environment, a\n","more flexible alternative to CityLearn, allows fast-timescale simula-\n","tion but does not provide a detailed thermal model of loads, options\n","for lockout or binary control, or classical baseline approaches to\n","compare with. High-frequency regulation has been addressed by\n","MARL, but only on the power generation side [ 57]. We are unaware\n","of any example in the literature deploying MARL for frequency\n","regulation with demand response, with second-timescale control\n","and flexible binary loads such as ACs which are subject to hardware\n","dynamic constraints like a lockout.\n","More generally, MARL has been developed for collaboration both\n","in virtual environments such as Dota 2 [ 44], Hide and Seek [ 8] or\n","Hanabi [ 20], and in real-world environments such as traffic light\n","control [ 55], single-house energy management [ 5] or ride-sharing\n","[46]. MARL problems pose several additional challenges to the RL\n","settings [ 21], such as the non-stationarity of the environment, the\n","need to learn coordination and communication, or the scaling of\n","the training and deployment. Multi-agent adaptations of known RL\n","algorithms, such as online PPO [ 48,60], or offline DDPG [ 36,38]\n","and DQN [ 42], have led to strong performance in many problems.\n","However, some particular problems, such as the ones requiring\n","communication with large numbers of agents, need specialized\n","algorithms [ 25]. TarMAC [ 17], for example, uses an attention mech-\n","anism to aggregate messages based on their importance.\n","3 PROBLEM FORMULATION\n","3.1 Environment\n","The environment is a simulation of an aggregation of 𝑁houses,\n","each equipped with a single air conditioning (AC) unit. The outdoor\n","temperature 𝑇𝑜,𝑡is assumed to be the same for every house, i.e., they\n","are co-located in the same geographical region, and is simulated as\n","sinusoidal with a one-day period. Unless otherwise specified, the\n","maximal temperature of 34 °C is reached at 6 pm and the minimal\n","temperature of 28 °C at 6 am.𝑇𝑜,𝑡is thus always above the target\n","indoor temperature 𝑇𝑇of 20 °C, so that every household can offer\n","its flexibility to the grid. The environment model is updated every\n","4 seconds. Thermostatic loads modeled as multi-zone units and\n","equipped with more than a single AC [ 7] is a topic for future work.\n","More details about the environment are given in Appendix C. A\n","notation table is provided in Appendix A.\n","3.1.1 House thermal model. Each house𝑖=1,2,...,𝑁 is simulated\n","using a second-order model based on Gridlab-D’s Residential mod-\n","ule user’s guide [ 24]. At time𝑡, the indoor air temperature 𝑇𝑖\n","ℎ,𝑡andthe mass temperature 𝑇𝑖\n","𝑚,𝑡are updated given the house character-\n","istics𝜃𝑖\n","𝑇(wall conductance 𝑈𝑖\n","ℎ, thermal mass 𝐶𝑖𝑚, air thermal mass\n","𝐶𝑖\n","ℎand mass surface conductance 𝐻𝑖𝑚), the outdoor temperature\n","𝑇𝑜,𝑡, and the heat 𝑄𝑖\n","𝑎,𝑡removed by the AC. By default, the thermal\n","characteristics are the same for each house and model a 100 square\n","meter, 1-floor house with standard isolation. During training and\n","deployment, the initial mass and air temperatures are set by adding\n","a positive random noise over the target temperature. Although it\n","is not used by default, the solar gain 𝑄𝑠,𝑡can also be added to the\n","simulation, as seen in Appendix C.1.1.\n","3.1.2 Air conditioners. Once again based on Gridlab-D’s guide [ 24],\n","air conditioner 𝑖’s heat removal capacity 𝑄𝑖\n","𝑎,𝑡and power consump-\n","tion𝑃𝑖\n","𝑎,𝑡are simulated based on the AC characteristics 𝜃𝑖𝑎, which\n","include their cooling capacity 𝐾𝑖𝑎, their coefficient of performance\n","𝐶𝑂𝑃𝑖𝑎and the latent cooling fraction 𝐿𝑖𝑎. The model and parameters\n","are also described in Appendix C.2. Additionally, a hard dynamic\n","constraint is set to protect the compressor: after being turned off,\n","it needs to wait a given amount of time before being allowed to\n","turn onagain [ 61]. This constraint is referred to as the lockout. By\n","default, the lockout duration 𝑙𝑖maxis set to 40 seconds.\n","3.1.3 Regulation signal. The power system operator sends to the\n","aggregator a signal 𝜌𝑡, which covers the complete aggregated load\n","consumption: the systems we cannot control such as computers,\n","washing machines, or lights, and the flexible power consumption, in\n","our case, the ACs. Let, 𝜌𝑡=𝐷𝑜,𝑡+𝑠𝑡where𝐷𝑜,𝑡is the power demand\n","for the non-controllable loads and 𝑠𝑡is the objective aggregated\n","AC power consumption, i.e., the flexible load. We define 𝐷𝑎,𝑡as\n","the power needed by the ACs to satisfy their thermal objectives,\n","i.e., to keep the temperature around the target. To focus on the\n","high-frequency variations of the power generation, we assume that\n","𝑠𝑡is well behaved at low frequencies, i.e., its mean in the 5 minutes\n","scale is𝐷𝑎,𝑡. A0-mean, high-frequency variation 𝛿𝑠,𝑡is added to\n","represent renewable intermittency the aggregator wants to mitigate.\n","We model the regulation signal as 𝑠𝑡=𝐷𝑎,𝑡+𝛿𝑠,𝑡.\n","The aggregation flexible power consumption is the sum of all of\n","the ACs’ consumption: 𝑃𝑡=Í𝑁\n","𝑖𝑃𝑖\n","𝑎,𝑡. The objective is to coordinate\n","the ACs in the aggregation so that 𝑃𝑡tracks𝑠𝑡.\n","Base signal. To compute the average needed power 𝐷𝑎,𝑡, we\n","created a dataset of the average power needed over a 5-minute\n","period by a bang-bang controller without lockout – which is optimal\n","for temperature – for all combinations of discrete sets of the relevant\n","parameters. At each time step, we interpolate the average power\n","demand of each AC from this dataset and sum them to compute\n","𝐷𝑎,𝑡. In practice, the base signal would be estimated or obtained\n","from historical data. The aggregator would then consider its value\n","when committing to track a signal 𝑠𝑡. This ensures that the required\n","power adjustment is enough to maintain the houses at acceptable\n","temperatures while providing flexibility to the grid.\n","Modelling high-frequency variations. The high-frequency varia-\n","tion𝛿𝑠,𝑡is modelled with 1-D Perlin noise [ 31], a smooth, proce-\n","durally generated 0-mean noise. The Perlin noise produces 𝛿𝑝,𝑡∈\n","[−1,1], and we have 𝛿𝑠,𝑡=𝐷𝑎,𝑡𝛽𝑝𝛿𝑝,𝑡where𝛽𝑝is an amplitude\n","parameter set to 0.9. Our Perlin noise is defined by 5 octaves and 5\n","octave steps per period of 400 seconds; it thus is the sum of noiseswith periods of 80, 40, 20, 10 and 5 seconds. More details are given\n","in Appendix C.3.2.\n","3.1.4 Communication between agents. To achieve coordination be-\n","tween agents, they must be able to communicate. For the agent im-\n","plementation to be decentralized, flexible, and privacy-preserving,\n","we consider limited and localized communications. This enables,\n","for example, devices communicating with simple radio-frequency\n","emitters, without the need for any further infrastructure. As such,\n","we limit the communication to a number 𝑁𝑐of neighbours. This\n","is in line with the low-deployment investment argument for using\n","demand response for frequency regulation.\n","3.2 Decentralized Partially Observable Markov\n","Decision Process\n","In this section, we formalize the above environment as a decentral-\n","ized, partially observable Markov decision process (Dec-POMDP)\n","characterized by the tuple ⟨S,A,O,P,R,𝛾⟩. LetSbe the global\n","state,A=Î𝑁\n","𝑖=1A𝑖the joint action space, and O=Î𝑁\n","𝑖=1O𝑖the\n","joint observation space. O𝑖partially observesS.Pdescribes the\n","environment’s transition probabilities, Rthe reward function for\n","each agent and 𝛾the discount parameter.\n","3.2.1 State, transition probabilities and actions. The state of the\n","environment 𝑋∈Sand its transition probabilities Pare unknown\n","to the agent. They are simulated by the environment dynamics\n","described in Section 3.1. Each agent 𝑖’s action𝑎𝑖\n","𝑡∈A𝑖is a binary\n","decision to control the AC status. If the remaining lockout time 𝑙𝑖\n","𝑡\n","is above zero, the onaction will be ignored by the AC. In practice,\n","a backup controller within the AC would prevent the ondecision\n","from being implemented.\n","3.2.2 Observations and communications. By default, agent 𝑖re-\n","ceives observation 𝑜𝑖\n","𝑡={𝑇𝑖\n","ℎ,𝑡,𝑇𝑖\n","𝑚,𝑡,𝑇𝑖\n","𝑇,𝜔𝑖\n","𝑡,𝑙𝑖\n","𝑡,𝑠𝑡/𝑁,𝑃𝑡/𝑁}at time\n","step𝑡, where𝑇𝑖\n","ℎ,𝑡,𝑇𝑖\n","𝑚,𝑡and𝑇𝑖\n","𝑇are the indoor air, mass, and target\n","temperatures, 𝜔𝑖\n","𝑡is the onoroffstatus of the AC, 𝑙𝑖\n","𝑡is its remain-\n","ing lockout time, 𝑠𝑡/𝑁is the per-agent regulation signal and 𝑃𝑡/𝑁\n","is the per-agent total consumption of the aggregation.\n","Each agent 𝑖communicates with its 𝑁𝑐neighbours. The mes-\n","sages’ sizes are not hard limited but should be small, and their\n","contents are not constrained. We define the set of all of agent 𝑖’s\n","𝑁𝑐neighbours as 𝑀𝑖. By default, we organize the agents in a 1-\n","dimensional structure: 𝑀𝑖={𝑖−⌊𝑁𝑐/2⌋,𝑖−⌊𝑁𝑐/2⌋+1,...,𝑖,...,𝑖+\n","⌊𝑁𝑐/2⌋−1,𝑖+⌊𝑁𝑐/2⌋}\\{𝑖}.\n","3.2.3 Reward. For each agent 𝑖, reward𝑟𝑖\n","𝑡is computed as the\n","weighted sum of the penalties due to its air temperature difference\n","with the target, which is unique to the agent, and to signal track-\n","ing, which is common across all agents. This scenario is therefore\n","cooperative with individual constraints. We normalize the reward\n","with𝛼temp=1and𝛼sig=3×10−7: a 0.5 °C error is penalized as\n","much as a 912 W per-agent error (each agent consumes 6000 W).\n","𝑟𝑖\n","𝑡=− \n","𝛼temp\u0010\n","𝑇𝑖\n","ℎ,𝑡−𝑇𝑖\n","𝑇,𝑡\u00112\n","+𝛼sig\u0012𝑃𝑡−𝑠𝑡\n","𝑁\u00132!4 CLASSICAL AND LEARNING-BASED\n","ALGORITHMS\n","4.1 Classical baselines\n","To the best of our knowledge, there is no classical baseline that\n","performs well under all the constraints enumerated in Section 1.\n","However, simple algorithms can optimize selected objectives, and\n","we use them as baselines for the results of the MARL agent.\n","4.1.1 Bang-bang controller. The bang-bang controller (BBC) turns\n","the AC onwhen the air temperature 𝑇𝑖\n","ℎ,𝑡is higher than the target\n","𝑇𝑖\n","𝑇, and offwhen it is lower. This is a decentralized algorithm,\n","which does not consider demand response but near-optimally con-\n","trols the temperature. When the lockout duration 𝑙𝑖maxis 0, the\n","BBC optimally controls the temperature, but does not account for\n","the signal. As the base signal 𝑠0,𝑡is computed to allow optimal\n","temperature control, BBC’s signal tracking error is mainly due to\n","the high-frequency variations of the signal.\n","4.1.2 Greedy myopic. The greedy controller is a centralized algo-\n","rithm that solves a knapsack problem [ 16] where the size of the\n","collection is the regulation signal, the weight of each AC is its con-\n","sumption𝑃𝑖\n","ℎ,𝑡, and its value is the temperature difference 𝑇𝑖\n","ℎ,𝑡−𝑇𝑖\n","𝑇.\n","At each time step, ACs are chosen based on a value priority com-\n","puted by(𝑇𝑖\n","ℎ,𝑡−𝑇𝑖\n","𝑇)/𝑃𝑖\n","ℎ,𝑡, until the aggregation’s consumption 𝑃𝑡is\n","higher than the regulation signal 𝑠𝑡. As it does not plan for the fu-\n","ture, the greedy myopic approach quickly runs out of available ACs\n","as most of them are in lockout. However, with a 0-lockout duration\n","𝑙𝑖max, it is optimal to track the signal 𝑠𝑡, and controls the temperature\n","in second priority. We implement the greedy myopic approach as it\n","is better adapted to these settings than the OO approach described\n","in Section 2. Indeed, OO only uses past state information and must\n","be implemented in a strictly online fashion. Both frameworks are\n","myopic, and struggle similarly with the lockout constraint.\n","4.1.3 Model predictive control. Model predictive control, or MPC,\n","is in its nominal form a centralized algorithm modeling the environ-\n","ment and identifying the actions which will lead to the highest sum\n","of rewards over a time horizon of 𝐻time steps. As the signal is sto-\n","chastic, MPC assumes a constant future signal over horizon 𝐻, and\n","optimally solves the trajectory with lockout. However, because it is\n","a large-scale combinatorial optimization problem, it scales poorly\n","with the number of agents 𝑁and with a horizon 𝐻. In the best\n","case the complexity is polynomial, but it is exponential in the worst\n","case. As a result, we were not able to run the MPC for more than 10\n","agents for𝐻=60s, and had to increase the time step between each\n","action to 12 seconds. More details are provided in Appendix D.1.\n","4.2 Learning-based methods\n","We deploy two algorithms using deep reinforcement learning, namely\n","MA-DQN and MA-PPO, both using the CT-DE paradigm. While MA-\n","DQN only uses hand-engineered communications, MA-PPO was im-\n","plemented with two communications paradigms: hand-engineered\n","and learned. Details about the architectures and hyperparameters\n","are provided in Appendix D.2.4.2.1 Centralized Training, Decentralized Execution. The CT-DE\n","paradigm [ 28] assumes that information is shared during the train-\n","ing of the agents, while they execute actions only based on their\n","decentralized observations. This reduces the non-stationarity of the\n","environment [ 21] and stabilizes the training. In our case, all agents\n","are homogeneous, which allows the use of parameter sharing [ 22].\n","As such, all ACs are controlled by identical instances of the same\n","policy trained from the shared experience of all agents.\n","4.2.2 MA-DQN. Multi-agent Deep Q-Network (MA-DQN) is the\n","CT-DE adaptation of DQN [ 42], an off-policy algorithm made for\n","discrete action spaces. A DQN agent mainly consists of a 𝑄-network\n","predicting the 𝑄-value of action-observation pairs (𝑎𝑖\n","𝑡,˜𝑜𝑖\n","𝑡)for every\n","possible𝑎𝑖\n","𝑡. During training, at time step 𝑡, the transition Θ𝑖\n","𝑡=\n","{˜𝑜𝑖\n","𝑡,𝑎𝑖\n","𝑡,𝑟𝑖\n","𝑡,˜𝑜𝑖\n","𝑡+1}of every agent is recorded in a common replay\n","buffer. This replay buffer is sampled to train the 𝑄-network to\n","predict𝑄(𝑎𝑖\n","𝑡,˜𝑜𝑖\n","𝑡)supervised with target 𝑇(𝑎𝑖\n","𝑡,˜𝑜𝑖\n","𝑡)according to Bell-\n","man’s optimality equation:\n","𝑇(𝑎𝑖\n","𝑡,˜𝑜𝑖\n","𝑡)=𝑟𝑖\n","𝑡+𝛾max𝑎𝑄(𝑎,˜𝑜𝑖\n","𝑡+1).\n","Actions are selected as 𝑎𝑖\n","𝑡with maximal predicted 𝑄-value given\n","an input ˜𝑜𝑖\n","𝑡.𝜖-greedy exploration is added during training.\n","4.2.3 MA-PPO. Multi-agent Proximal Policy Optimization (MA-\n","PPO) [ 60] is the CT-DE adaptation of clipped PPO [ 48], an on-\n","policy, policy-gradient algorithm. The agent jointly learns a policy\n","𝜋𝜃(𝑎𝑖\n","𝑡|˜𝑜𝑖\n","𝑡), also called an actor, and a value function 𝑉𝜙(˜𝑜𝑖\n","𝑡), also\n","called a critic. At each epoch, the policy is fixed and the transitions\n","Θ𝑖\n","𝑡={˜𝑜𝑖\n","𝑡,𝑎𝑖\n","𝑡,𝜋𝜃𝑡(𝑎𝑖\n","𝑡|˜𝑜𝑖\n","𝑡),𝑟𝑖\n","𝑡}for all agents are recorded together for\n","one or several episodes of length 𝐻. For each Θ𝑖\n","𝑡, a return𝐺𝑖\n","𝑡=Í𝐻−𝑡\n","𝜏=0𝛾𝜏𝑟𝑖\n","𝑡+𝜏is computed based on future experience. Then, the\n","new policy parameters 𝜃𝑡+1are trained over the stored memory to\n","optimize the clipped PPO objective L(˜𝑜𝑖\n","𝑡,𝑎𝑖\n","𝑡,𝜃𝑡+1,𝜃𝑡), maximizing\n","the advantage 𝐴𝜋𝜃𝑡(˜𝑜𝑖\n","𝑡,𝑎𝑖\n","𝑡)=𝐺𝑖\n","𝑡−𝑉𝜙(˜𝑜𝑖\n","𝑡)under the constraint of\n","proximity around the previous policy. The critic parameters 𝜙are\n","then trained so that 𝑉𝜙(˜𝑜𝑖\n","𝑡)predicts the return 𝐺𝑖\n","𝑡. The memory is\n","erased and a new epoch starts.\n","Exploration is handled by the inherent stochasticity of the policy.\n","In the CT-DE setting, 𝑉𝜙, which is only used during training, is\n","given additional information about the states of other agents.\n","4.2.4 Communications.\n","Hand-engineered communications. For MA-DQN and the hand-\n","engineered MA-PPO, the messages are designed based on the state\n","of each agent, effectively providing a wider observability of the gen-\n","eral state. Agent 𝑗’s message𝑚𝑗,𝑡contains the current difference\n","between its air and target temperatures 𝑇𝑗\n","ℎ,𝑡−𝑇𝑗\n","𝑇, its remaining lock-\n","out time𝑙𝑗\n","𝑡, and its current status 𝜔𝑗\n","𝑡. The messages{𝑚𝑖\n","𝑗,𝑡}∀𝑗∈𝑀𝑖\n","from agents 𝑗∈𝑀𝑖are concatenated with the observations 𝑜𝑖\n","𝑡to\n","create the input ˜𝑜𝑖\n","𝑡of the neural networks. Message 𝑚𝑖\n","𝑗,𝑡from agent\n","𝑗to agent𝑖is at a fixed place in the ˜𝑜𝑖\n","𝑡vector based on its relative\n","position𝑖−𝑗. MA-PPO with hand-engineered communication will\n","be referred to as MA-PPO-HE.\n","Targeted Multi-Agent Communication. To allow agents to learn\n","to communicate, we implement TarMAC [ 17] in MA-PPO. TarMACis an attention-based targeted communication algorithm where\n","each agent outputs a key, a message and a query. The key is sent\n","along with the message to the other agents, which then multiply it\n","with their query to compute the attention they give to the message.\n","All messages are then aggregated using the attention as a weight.\n","The three modules – key, message, query – are trained. TarMAC\n","allows more flexibility to the agents: it does not restrict the contents\n","of the communication, and it allows agents to communicate with a\n","different number of houses than they were communicating with\n","during training. More details are available in Appendix D.2.1. We\n","refer to this version as TarMAC-PPO.\n","No communication. It is also possible to train agents without\n","communication. In this case, it only observes 𝑜𝑖\n","𝑡. This agent is\n","referred to as MA-PPO-NC.\n","4.2.5 Agent training. The learning agents were trained on envi-\n","ronments with 𝑁tr={10,20,50}houses and communicating with\n","𝑁ctr={9,19,49}other agents. We trained every agent on 16 differ-\n","ent seeds: 4 for environment and 4 for network initialization. They\n","were trained on 3286800 time steps, equivalent to 152 days, divided\n","in 200 episodes. Each episode is initialized with each house having\n","a temperature higher than the target, sampled from the absolute\n","value of a 0-mean Gaussian distribution with 𝜎=5°C. We tuned the\n","hyperparameters through a grid search, as shown in Appendix D.\n","The contribution of this paper is to demonstrate that learning-based\n","methods can lead to high performance on the problem of high fre-\n","quency regulation. We therefore do not compile statistics over the\n","trained agents; instead, for each situation, we select the two best\n","agents over the seeds based on test return, and report the best score\n","from these two on the benchmark environment.\n","5 RESULTS AND ANALYSIS\n","5.1 Metrics of performance\n","We deploy the agents on a benchmark environment with 𝑁dehouses\n","on trajectories of 43200 steps, i.e., two full days. We evaluate their\n","performance with the per-agent root mean square error (RMSE)\n","between the regulation signal 𝑠𝑡and aggregated power consump-\n","tion𝑃𝑡. We also measure the temperature RMSEs – one for all\n","agents, one of the maximal temperature error of the aggregation –\n","to ensure thermal control. Every house’s temperature is initialized\n","differently, so we start computing the RMSE when the temperature\n","is controlled, after 5000 steps. For context, a single AC consumes\n","6000 W when turned on. Due to the MPC’s computing time, its\n","performance is evaluated differently, as explained in Appendix D.1.\n","Unless mentioned otherwise, the results are the mean and standard\n","deviation over 10 environmental seeds.\n","5.2 Performance of agents\n","Table 1 shows the performance of different agents in environments\n","with and without lockout with 𝑁deof 10, 50, 250 and 1000 houses.\n","The per-agent signal RMSE generally goes down when 𝑁dein-\n","creases. This is due to the lower relative discretization error, but\n","also because, with more agents, errors have more chances to cancel\n","each other, as explained in Appendix E. As expected, BBC controls\n","the temperature well, but does not track the signal. Without lock-\n","out, the greedy myopic shows near-optimal signal tracking, whereerrors are due to discretization. It also maintains good control of\n","the temperature. With lockout, however, it fails, as it runs out of\n","available agents. The MPC gives good results for 10 agents, but its\n","performance is limited by the lower control frequency of 12 seconds.\n","It could not be run on 𝑁de=50for computing time reasons. DQN\n","controls the temperature well but is only slightly better than BBC\n","on the signal. Both PPO agents show significantly better perfor-\n","mance, and TarMAC-PPO outperforms MA-PPO-HE at high 𝑁de.\n","The results without communication will be discussed in Section 5.5.\n","Figure 1 shows the behaviour of each agent over two days for\n","50 houses. Every point on the curves is averaged over 10 minutes.\n","The mean offset captures the error’s bias by averaging the differ-\n","ences such that positives and negatives cancel each other, while\n","the mean error is the mean of the absolute differences. The signal\n","and consumption curves start very high due to the initial situation,\n","and then follow the sinusoidal pattern of the outdoor temperature.\n","Without lockout, the BBC shows low temperature and signal offsets,\n","with a significant signal error, as it does not track high-frequency\n","variations of the signal. With the lockout, it under-consumes as\n","explained in Section 4.1.1, leading to a positive temperature offset,\n","and the base signal rises to compensate. As the signal variation\n","amplitude is high, this does not strongly affect the error.\n","The DQN agent has a smaller signal offset and error, especially at\n","night when the amplitude of the signal variations is lower. During\n","the day, the signal error is still significant. Both MA-PPO agents, on\n","the other hand, have a near-0 offset in signal and temperature. Their\n","signal error is also significantly lower than the others, because they\n","are able to track the high-frequency variations.\n","5.3 Scalability with number of agents\n","As shown in Table 1, the PPO agents, and TarMAC-PPO especially,\n","scale gracefully with the number of agents. Figure 2 shows the\n","consumption and signal over 800 seconds for agents deployed over\n","𝑁𝑑=50and 1000 over 800seconds. For 𝑁𝑑=50, the agents do not\n","perfectly match the signal. However, the same agent does better\n","on 1000 houses. Indeed, as the environment is homogeneous, the\n","local strategy scales smoothly by averaging out errors. The best\n","performing agents for TarMAC-PPO were trained on environments\n","with𝑁tr=10 houses. With MA-PPO-HE, it is often the agents\n","trained on𝑁tr=20 that had the best results. Training with 𝑁tr=50\n","probably makes the credit assignment harder as shown in Figure 3.\n","5.4 PPO agents’ dynamics\n","As visualized in Figure 4, both MA-PPO-HE and TarMAC-PPO\n","policies keep the ACs in lockout or on, and never off. This is\n","optimal for temperature control: an agent needing to be offto\n","warm up after lockout, would not have had the time to warm up\n","during the lockout and was thus onfor too long beforehand. The\n","agents turn onas soon as they can, but control when they turn off\n","based on the context and the messages of other agents.\n","A fascinating feature of the learned policies is the cyclic be-\n","haviour used by MA-PPO-HE agents for coordination. As shown\n","in Figure 4, the ACs turn onone after the other based on their\n","positions in the aggregation, with a repetitive pattern. This hap-\n","pened for each MA-PPO-HE agent we trained, although the pattern\n","period or moving direction was different. These patterns enableTable 1: Performance of the different agents, computed over 10 environment seeds.\n","𝑁de=10 𝑁de=50 𝑁de=250 𝑁de=1000\n","Per-agent Signal T. Max T. Signal T. Max T. Signal T. Max T. Signal T. Max T.\n","RMSE (W) (°C) (°C) (W) (°C) (°C) (W) (°C) (°C) (W) (°C) (°C)\n","No l.oGreedy 194±1 0.04 0.06 70±1 0.03 0.05 63±1 0.03 0.052 63±1 0.03 0.05\n","BBC 806±147 0.02 0.03 392±50 0.02 0.04 310±11 0.02 0.03 272±12 0.02 0.03\n","40s l.oGreedy 2668±14 0.87 0.93 3166±12 1.09 1.15 3313±12 1.16 1.22 3369±15 1.18 1.24\n","BBC 830±207 0.05 0.09 426±63 0.05 0.10 318±7 0.05 0.10 296±4 0.05 0.10\n","MPC 344±96 0.07 0.12 - - - - - - - - -\n","MA-DQN 541±86 0.05 0.09 321±24 0.05 0.10 246±8 0.05 0.11 234±4 0.05 0.12\n","MA-PPO-HE 253±1 0.04 0.08 161±8 0.04 0.08 127±2 0.04 0.11 122±3 0.05 0.13\n","TarMAC-PPO 247±3 0.04 0.07 158±20.04 0.09 115±10.05 0.13 101±20.05 0.14\n","MA-PPO-NC 434±2 0.06 0.08 215±1 0.06 0.14 132±1 0.06 0.16 107±1 0.06 0.17\n","Figure 1: MA-PPO-HE and TarMAC-PPO outperform DQN and BBC for signal and temperature over 2 days with 𝑁de=50\n","agents.\n","Figure 2: Both MA-PPO policies scale seamlessly in the number of agents: signal and consumption on 800s for 𝑁de=50and\n","1000.\n","Figure 3: Training with more agents 𝑁trdoes not lead to bet-\n","ter performance, even when deployed on large 𝑁de.\n","agent coordination thanks to the stable message structure, i.e., the\n","fixed relative position of agent 𝑗’s message to agent 𝑖in the ˜𝑜𝑖\n","𝑡\n","vector. The TarMAC-PPO agents, on the other hand, do not follow\n","a pattern in their collective behavior. Indeed, aggregated messages\n","do not contain information about the structure of the neighbours.\n","The coordination is done through flexible message contents.\n","5.5 Communications\n","The agents need communications to coordinate and get the best\n","results. Intuitively, the more agents to communicate with, the betterthe performance because the observability of the environment is\n","improved. In practice, this is not always the case, as shown in\n","Figure 5. For TarMAC-PPO, communicating with 9 neighbours\n","often leads to the best performance. Higher values of 𝑁cdecan\n","lead to a reduction of the weight of important messages in the\n","aggregation. For MA-PPO-HE, communicating with 19 agents yields\n","better results than with 49. Indeed, in MA-PPO-HE, the agents\n","must have𝑁cde=𝑁ctr. During training, communicating with more\n","agents increases the credit assignment difficulty as it increases\n","the input size with non-controllable elements. It is also clear in\n","Figure Figure 5 that agents trained to communicate do not cope\n","well when not communicating. Figure 6 shows the performance\n","of a TarMAC agent trained with 𝑁tr=10and𝑁ctr=9on an\n","environment with 𝑁de=50agents, when changing the number\n","𝑁cdeof neighbours it can communicate with. The performance is\n","bad at low communication but stabilizes around 7 or 8 agents.\n","It is, however, possible to train an agent without communication\n","to do better than Bang-Bang control, as shown by the performance\n","of MA-PPO-NC in Table 1. Without coordinating with the others,(a) MA-PPO-HE\n"," (b) TarMAC-PPO\n","Figure 4: State of 20 houses controlled with two different PPO agents. The number on the top right is the remaining lockout\n","time. (Left) Two different agents of MA-PPO-HE with 𝑁cde=19show a “20-house” (up) and a “3-house” (down) pattern. (Right)\n","Two different TarMAC-PPO agents show no such pattern.\n","Figure 5: TarMAC-PPO’s performance does not increase af-\n","ter𝑁cde=9, while MA-PPO-HE is better with 𝑁cde=19, for\n","𝑁de=250agents.\n","Figure 6: A TarMAC-PPO agent performs well as long as it\n","communicates with 𝑁cde=7agents or more, on 𝑁de=50.\n","an agent can learn to act well on average to minimize the signal\n","error. When there are only a few agents, as when 𝑁de=10or 50,\n","this does not perform very well. However, the performance gap\n","decreases when 𝑁deincreases: a good average policy will do well\n","when applied on many agents. Another way to see this is that, with\n","large𝑁de, each agent’s importance becomes negligible in the final\n","result. As such, the group can be seen as a single average agent,\n","and the problem cam be posed as a mean field game [50, 59].\n","Interestingly, MA-PPO-HE at high 𝑁dedoes better with commu-\n","nication defects. This may be because the MA-PPO-HE coordina-\n","tion leads to locally biased policies, which do not benefit from the\n","averaging effect reducing the relative error when 𝑁deincreases.\n","5.6 Robustness\n","All the results presented were produced under certain assumptions,\n","such as homogeneous houses and ACs, consistent outdoor tem-\n","perature and signal profiles, and faultless communication. If suchagents were to be deployed in the real world, they would be con-\n","fronted with situations where these conditions are not satisfied. In\n","this section, we evaluate the robustness of our trained agents to\n","different disturbances in the deployment conditions.\n","5.6.1 Faulty communications. As previously demonstrated, com-\n","munications are key for good performance of the agents. In this\n","robustness test, we simulate defective communications. At every\n","time step, each message 𝑚𝑖\n","𝑗is defective with a probability 𝑝𝑑. In the\n","case of TarMAC-PPO, this leads to the message not being received.\n","For MA-PPO-HE, every element of the message is set to 0. We tested\n","the best agents for 𝑁de=10, 50, 250 and 1000 houses with 𝑝𝑑=0.1\n","and 0.5, as seen in Table 2. MA-PPO-HE agents’ coordination is\n","based on their stable communication structure. As a result, it copes\n","badly with defective communications. Interestingly, when 𝑁deis\n","higher, the impact decreases, even leading to better performance at\n","𝑁de=1000. This may be due to the fact that the resulting policies\n","cannot coordinate locally and are less locally biased. The TarMAC-\n","PPO handles perfectly temporary defects in communication as its\n","messages are aggregated. This is the case even with 𝑝𝑑=0.5and\n","when the agent communicates with 𝑁ctr=9neighbours only.\n","5.6.2 Heterogeneous houses and ACs. In reality, different houses\n","have different thermal characteristics. The ACs also do not always\n","have the same rated power or lockout duration. We deployed the\n","best trained MA-PPO-HE and TarMAC-PPO agents for 50-house\n","environments that do not comply with these assumptions, to evalu-\n","ate their robustness to separate disturbances. We also trained new\n","agents on environments with these conditions, to allow the agents\n","to learn to cope with heterogeneity. The relevant characteristics\n","were observed by both agents as part of 𝑜𝑖\n","𝑡, and of the messages\n","𝑚𝑖\n","𝑗in MA-PPO-HE. These agents are referred to with the -T suffix.\n","The thermal characteristics heterogeneity was simulated by adding\n","a Gaussian noise to each element of 𝜃𝑖\n","ℎfor each house, with a stan-\n","dard deviation of 50% of the original value (the final values cannot\n","be negative). For the ACs cooling capacities 𝐾𝑖𝑎, a value between\n","10, 12.5, 15, 17.5 and 20 kW was uniformly selected for each house.\n","Finally, heterogeneity in the lockout duration 𝑙maxwas tested by\n","sampling uniformly between 32, 36, 40, 44 and 48 seconds.\n","The results are shown in Table 3. TarMAC-PPO is much more\n","robust to heterogeneity in agents than MA-PPO-HE. This is becauseTable 2: Performance under faulty communication (5 seeds)\n","𝑁de=10 𝑁de=50 𝑁de=250 𝑁de=1000\n","Per-agent Signal T. Max T. Signal T. Max T. Signal T. Max T. Signal T. Max T.\n","RMSE (W) (°C) (°C) (W) (°C) (°C) (W) (°C) (°C) (W) (°C) (°C)\n","MA-PPO-HE𝑝𝑑=0 253±10.04 0.08 161±80.04 0.08 127±20.04 0.11 122±30.05 0.13\n","𝑝𝑑=0.1504±20.07 0.14 207±10.04 0.11 138±20.05 0.13 118±10.05 0.14\n","𝑝𝑑=0.5597±20.10 0.19 274±10.06 0.15 148±10.06 0.151 115±20.06 0.17\n","TarMAC-PPO𝑝𝑑=0 247±30.04 0.07 158±20.04 0.09 115±10.05 0.13 101±20.05 0.14\n","𝑝𝑑=0.1246±20.04 0.07 158±20.04 0.09 115±20.05 0.12 101±10.05 0.14\n","𝑝𝑑=0.5248±20.04 0.07 159±30.04 0.09 115±20.05 0.13 101±10.05 0.14\n","Table 3: Performance under house and AC heterogeneity\n","MA-PPO-HE MA-PPO-HE-T\n","Per-agent Signal Max T. Signal Max T.\n","RMSE (W) (°C) (W) (°C)\n","Homogeneous 161±8 0.08 - -\n","House thermal 285±8 0.17 222±7 0.11\n","AC cooling 292±3 0.15 181±3 0.14\n","Lockout duration 324±9 0.15 246±4 0.09\n","TarMAC-PPO TarMAC-PPO-T\n","Homogeneous 158±2 0.09 - -\n","House thermal 184±2 0.12 174±2 0.11\n","AC cooling 187±2 0.16 185±9 0.16\n","Lockout duration 192±3 0.09 251±4 0.08\n","Table 4: Robustness on environment changes (5 seeds)\n","MA-PPO-HE TarMAC-PPO\n","Per-agent Signal Max T. Signal Max T.\n","RMSE (W) (°C) (W) (°C)\n","Same as training 161±8 0.08 158±2 0.09\n","Solar gain 190±6 0.09 174±2 0.10\n","Outdoor T. + 4 °C 203±4 0.11 198±2 0.11\n","Outdoor T. - 4 °C 170±1 0.09 184±2 0.12\n","Signal average + 30% 401±2 0.11 302±2 0.14\n","Signal average - 30% 337±4 0.10 317±1 0.11\n","Signal noise amplitude + 30% 188±5 0.08 179±3 0.09\n","Signal noise frequency + 100% 200±4 0.08 198±5 0.09\n","in MA-PPO-HE the coordination scheme is based on the stable\n","dynamics of the agent’s neighbours, especially with the lockout\n","duration. TarMAC-PPO is instead more flexible with respect to\n","different dynamics. For both agents, it is possible to reduce the effect\n","of heterogeneity by training the agents on such environments and\n","allowing them to observe the characteristics. This is different for\n","heterogenity on the lockout duration, where TarMAC-PPO did not\n","seem able to train satisfactorily on such conditions. An interesting\n","observation is that the best TarMAC-PPO results were obtained\n","when communicating with 𝑁ctr=49agents. With heterogeneous\n","agents, more neighbours are needed for a representative input.\n","5.6.3 Other environments. We also tested our agents on environ-\n","ments differing from the training environment, with different out-\n","door temperature 𝑇𝑜, solar gain 𝑄𝑠, too low or high average sig-\n","nal𝐷𝑎, and higher or faster signal variations 𝛿𝑠. As can be seen\n","in Table 4, both agents are quite robust to such changes, with\n","TarMAC-PPO usually leading to better results. When the signal\n","is misbehaved, i.e., it is too low or too high to allow correct con-\n","trol of the temperature, there is a tradeoff between the signal andthe temperature objectives. MA-PPO-HE gives higher priority to\n","temperature, leading to higher signal RMSE.\n","5.7 Processing time\n","In Table 5, we report the processing time for action selection of the\n","baseline and trained agents. The results are shown for 25 times steps\n","(100 seconds of simulation), except for the MPC which simulated\n","100 seconds with 10-time steps. They were computed on the 12-core,\n","2.2 GHz Intel i7-8750H CPU of a laptop computer.\n","Table 5: Computation time (s) for action selection, for 100\n","seconds of simulation. We report the time per-agents for a\n","decentralized system and for the whole system otherwise.\n","Agent Decentralized 𝑁de=10𝑁de=1000\n","TarMAC-PPO Yes 0.002 0.001\n","MA-PPO-HE Yes 0.006 0.006\n","DQN Yes 0.003 0.002\n","BBC Yes 0.00001 0.00001\n","Greedy myopic No 0.1 3.7\n","MPC -𝐻=40s No 92.6680 -\n","As the decentralized, learned agents only need a single forward\n","pass in a relatively small neural network, the time for action se-\n","lection is sufficiently low for control when using 4-second time\n","steps. Centralized approaches such as greedy myopic scale badly\n","with many agents. MPC, already simplified with time steps of 12\n","seconds instead of 4, and a short horizon of 40 seconds, takes an\n","unacceptable amount of time for more than 10 agents.\n","6 CONCLUSION\n","In this paper, we tackle the problem of high-frequency regulation\n","with demand response by controlling discrete and dynamically\n","constrained residential loads equipped with air conditioners with\n","a decentralized, real-time agent trained by MA-PPO. We test two\n","frameworks for local communication – fixed hand-engineered mes-\n","sages and learned targeted communication. The policies trained\n","with few agents perform significantly better than baselines, scale\n","seamlessly to large numbers of houses, and are robust to most dis-\n","turbances. Our results show that MARL can be used successfully\n","to solve some of the complex multi-agent problems induced by the\n","integration of renewable energy in electrical power grids. Future\n","works towards the application of such algorithms on real power sys-\n","tems could include sim2real transfer, integration of more complex\n","flexible loads, as well as power grid safety issues.REFERENCES\n","[1]International Energy Agency. [n.d.]. Energy Statistics Data Browser – Data\n","Tools. Available on: https://www.iea.org/data-and-statistics/data-tools/energy-\n","statistics-data-browser (Accessed on Sept 15, 2022).\n","[2]International Energy Agency. 2018. The Future of Cooling . https://www.iea.org/\n","reports/the-future-of-cooling\n","[3]International Energy Agency. 2021. Greenhouse Gas Emissions from Energy:\n","Overview .\n","[4]Roya Ahmadiahangar, Tobias Häring, Argo Rosin, Tarmo Korõtko, and João\n","Martins. 2019. Residential Load Forecasting for Flexibility Prediction Using\n","Machine Learning-Based Regression Model. In 2019 IEEE International Con-\n","ference on Environment and Electrical Engineering and 2019 IEEE Industrial\n","and Commercial Power Systems Europe (EEEIC / I&CPS Europe) . 1–4. https:\n","//doi.org/10.1109/EEEIC.2019.8783634\n","[5]Mehdi Ahrarinouri, Mohammad Rastegar, and Ali Reza Seifi. 2021. Multiagent\n","Reinforcement Learning for Energy Management in Residential Buildings. IEEE\n","Transactions on Industrial Informatics 17, 1 (Jan 2021), 659–666. https://doi.org/\n","10.1109/TII.2020.2977104\n","[6]Sally Aladdin, Samah El-Tantawy, Mostafa M. Fouda, and Adly S. Tag Eldien.\n","2020. MARLA-SG: Multi-Agent Reinforcement Learning Algorithm for Efficient\n","Demand Response in Smart Grid. IEEE Access 8 (2020), 210626–210639. https:\n","//doi.org/10.1109/ACCESS.2020.3038863\n","[7]Uzma Amin, MJ Hossain, and E Fernandez. 2020. Optimal price based control\n","of HVAC systems in multizone office buildings for demand response. Journal of\n","Cleaner Production 270 (2020), 122059.\n","[8]Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob\n","McGrew, and Igor Mordatch. 2020. Emergent Tool Use From Multi-Agent Au-\n","tocurricula. (Feb 2020). arXiv:1909.07528 [cs, stat].\n","[9]Hassan Bevrani, Arindam Ghosh, and Gerard Ledwich. 2010. Renewable energy\n","sources and frequency regulation: survey and new perspectives. IET Renewable\n","Power Generation 4, 5 (2010), 438–457.\n","[10] David Biagioni, Xiangyu Zhang, Dylan Wald, Deepthi Vaidhynathan, Rohit\n","Chintala, Jennifer King, and Ahmed S. Zamzam. 2021. PowerGridworld: A\n","Framework for Multi-Agent Reinforcement Learning in Power Systems. https:\n","//doi.org/10.48550/ARXIV.2111.05969\n","[11] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\n","Schulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym.\n","arXiv:arXiv:1606.01540\n","[12] Duncan S Callaway. 2009. Tapping the energy storage potential in electric loads\n","to deliver load following and regulation, with application to wind energy. Energy\n","Conversion and Management 50, 5 (2009), 1389–1400.\n","[13] Bingqing Chen, Jonathan Francis, Marco Pritoni, Soummya Kar, and Mario Bergés.\n","2020. Cohort: Coordination of heterogeneous thermostatically controlled loads\n","for demand flexibility. In Proceedings of the 7th ACM International Conference on\n","Systems for Energy-Efficient Buildings, Cities, and Transportation . 31–40.\n","[14] Bingqing Chen, Jonathan Francis, Marco Pritoni, Soummya Kar, and Mario Bergés.\n","2020. COHORT: Coordination of Heterogeneous Thermostatically Controlled\n","Loads for Demand Flexibility. In Proceedings of the 7th ACM International Confer-\n","ence on Systems for Energy-Efficient Buildings, Cities, and Transportation . 31–40.\n","https://doi.org/10.1145/3408308.3427980 arXiv:2010.03659 [cs, eess].\n","[15] CIBSE. 2015. Guide A: Environmental Design (8th ed.). Chartered Institution of\n","Building Services Engineers.\n","[16] George B. Dantzig. 1957. Discrete-Variable Extremum Problems. Operations\n","Research 5, 2 (Apr 1957), 266–288. https://doi.org/10.1287/opre.5.2.266\n","[17] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike\n","Rabbat, and Joelle Pineau. 2019. TarMAC: Targeted Multi-Agent Communication.\n","InProceedings of the 36th International Conference on Machine Learning . PMLR,\n","1538–1546. https://proceedings.mlr.press/v97/das19a.html\n","[18] Steven Diamond and Stephen Boyd. 2016. CVXPY: A Python-Embedded Modeling\n","Language for Convex Optimization. Journal of Machine Learning Research (2016).\n","https://stanford.edu/~boyd/papers/pdf/cvxpy_paper.pdf To appear.\n","[19] Ivana Dusparic, Colin Harris, Andrei Marinescu, Vinny Cahill, and Siobhán\n","Clarke. 2013. Multi-agent residential demand response based on load forecasting.\n","In2013 1st IEEE Conference on Technologies for Sustainability (SusTech) . 90–96.\n","https://doi.org/10.1109/SusTech.2013.6617303\n","[20] Andrew Fuchs, Michael Walton, Theresa Chadwick, and Doug Lange. 2021.\n","Theory of Mind for Deep Reinforcement Learning in Hanabi. (Jan 2021).\n","http://arxiv.org/abs/2101.09328 arXiv:2101.09328 [cs].\n","[21] Sven Gronauer and Klaus Diepold. 2021. Multi-agent deep reinforcement learning:\n","a survey. Artificial Intelligence Review (Apr 2021). https://doi.org/10.1007/s10462-\n","021-09996-w\n","[22] Jayesh K. Gupta, Maxim Egorov, and Mykel Kochenderfer. 2017. Cooperative Multi-\n","agent Control Using Deep Reinforcement Learning . Lecture Notes in Computer\n","Science, Vol. 10642. Springer International Publishing, Cham, 66–83. https:\n","//doi.org/10.1007/978-3-319-71682-4_5\n","[23] Gurobi Optimization, LLC. 2022. Gurobi Optimizer Reference Manual. https:\n","//www.gurobi.com[24] Betelle Memorial Institute. [n.d.]. GridLAB-D Wiki. http://gridlab-d.shoutwiki.\n","com/wiki/Main_Page http://gridlab-d.shoutwiki.com/wiki/Main_Page (Accessed\n","on: Sept 15, 2022).\n","[25] Jiechuan Jiang and Zongqing Lu. 2018. Learning Attentional Communication for\n","Multi-Agent Cooperation. In Advances in Neural Information Processing Systems ,\n","Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/hash/\n","6a8018b3a00b69c008601b8becae392b-Abstract.html\n","[26] Jin, Mohammed Olama, Teja Kuruganti, James Nutaro, Christopher Winstead,\n","Yaosuo Xue, and Alexander Melin. 2018. Model Predictive Control of Building\n","On/Off HVAC Systems to Compensate Fluctuations in Solar Power Generation.\n","In2018 9th IEEE International Symposium on Power Electronics for Distributed\n","Generation Systems (PEDG) . 1–5. https://doi.org/10.1109/PEDG.2018.8447840\n","[27] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Opti-\n","mization. arXiv:1412.6980 [cs] (Jan 2017). http://arxiv.org/abs/1412.6980 arXiv:\n","1412.6980.\n","[28] Landon Kraemer and Bikramjit Banerjee. 2016. Multi-agent reinforcement learn-\n","ing as a rehearsal for decentralized planning. Neurocomputing 190 (May 2016),\n","82–94. https://doi.org/10.1016/j.neucom.2016.01.031\n","[29] Prabha Kundur. 2007. Power system stability. Power system stability and control\n","(2007), 7–1.\n","[30] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.\n","2019. Quantifying the Carbon Emissions of Machine Learning. arXiv:1910.09700\n","(Nov 2019). https://doi.org/10.48550/arXiv.1910.09700 arXiv:1910.09700 [cs].\n","[31] A. Lagae, S. Lefebvre, R. Cook, T. DeRose, G. Drettakis, D.S. Ebert, J.P. Lewis, K.\n","Perlin, and M. Zwicker. 2010. A Survey of Procedural Noise Functions. Computer\n","Graphics Forum 29, 8 (Dec 2010), 2579–2600. https://doi.org/10.1111/j.1467-\n","8659.2010.01827.x\n","[32] Fiorella Lauro, Fabio Moretti, Alfonso Capozzoli, and Stefano Panzieri. 2015.\n","Model Predictive Control for Building Active Demand Response Systems. Energy\n","Procedia 83 (2015), 494–503. https://doi.org/10.1016/j.egypro.2015.12.169 Sustain-\n","ability in Energy and Buildings: Proceedings of the 7th International Conference\n","SEB-15.\n","[33] Young M Lee, Raya Horesh, and Leo Liberti. 2015. Optimal HVAC control as\n","demand response with on-site energy storage and generation system. Energy\n","Procedia 78 (2015), 2106–2111.\n","[34] Antoine Lesage-Landry and Joshua A Taylor. 2018. Setpoint tracking with\n","partially observed loads. IEEE Transactions on Power Systems 33, 5 (2018), 5615–\n","5627.\n","[35] Antoine Lesage-Landry, Joshua A Taylor, and Duncan S Callaway. 2021. Online\n","Convex Optimization with Binary Constraints. IEEE Trans. Automat. Control\n","(2021).\n","[36] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom\n","Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. Continuous control\n","with deep reinforcement learning. (Jul 2019). arXiv:1509.02971 [cs, stat].\n","[37] M Liu and Y Shi. 2015. Model predictive control of aggregated heterogeneous\n","second-order thermostatically controlled loads for ancillary services. IEEE Trans.\n","on Power Systems 31, 3 (2015), 1963–1971.\n","[38] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. 2020.\n","Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.\n","(Mar 2020). http://arxiv.org/abs/1706.02275 arXiv:1706.02275 [cs].\n","[39] Mehdi Maasoumy, Borhan M Sanandaji, Alberto Sangiovanni-Vincentelli, and\n","Kameshwar Poolla. 2014. Model predictive control of regulation services from\n","commercial buildings to the smart grid. In 2014 American Control Conference .\n","IEEE, 2226–2233.\n","[40] J L Mathieu, S Koch, and D S Callaway. 2012. State estimation and control\n","of electric loads to manage real-time energy imbalance. IEEE Trans. on Power\n","Systems 28, 1 (2012), 430–440.\n","[41] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timo-\n","thy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-\n","chronous Methods for Deep Reinforcement Learning. arXiv:1602.01783 [cs] (Jun\n","2016). http://arxiv.org/abs/1602.01783 arXiv: 1602.01783.\n","[42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,\n","Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg\n","Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\n","King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.\n","Human-level control through deep reinforcement learning. Nature 518, 75407540\n","(Feb 2015), 529–533. https://doi.org/10.1038/nature14236\n","[43] Mohammed M. Olama, Teja Kuruganti, James Nutaro, and Jin Dong. 2018. Coordi-\n","nation and Control of Building HVAC Systems to Provide Frequency Regulation\n","to the Electric Grid. Energies 11, 7 (2018). https://doi.org/10.3390/en11071852\n","[44] OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Prze-\n","mysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,\n","Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki,\n","Michael Petrov, Henrique P. d O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy\n","Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski,\n","and Susan Zhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning.\n","(Dec 2019). http://arxiv.org/abs/1912.06680 arXiv: 1912.06680.[45] Aisling Pigott, Constance Crozier, Kyri Baker, and Zoltan Nagy. 2021. GridLearn:\n","Multiagent Reinforcement Learning for Grid-Aware Building Energy Manage-\n","ment. https://doi.org/10.48550/ARXIV.2110.06396\n","[46] Zhiwei Qin, Hongtu Zhu, and Jieping Ye. 2022. Reinforcement Learning for\n","Ridesharing: An Extended Survey. (Aug 2022). http://arxiv.org/abs/2105.01099\n","arXiv:2105.01099 [cs].\n","[47] Martin Roesch, Christian Linder, Roland Zimmermann, Andreas Rudolf, Andrea\n","Hohmann, and Gunther Reinhart. 2020. Smart Grid for Industry Using Multi-\n","Agent Reinforcement Learning. Applied Sciences 10, 19 (2020). https://doi.org/\n","10.3390/app10196900\n","[48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n","2017. Proximal Policy Optimization Algorithms. (2017). https://doi.org/10.1007/\n","s00038-010-0125-8\n","[49] Pierluigi Siano. 2014. Demand response and smart grids—A survey. Renewable\n","and sustainable energy reviews 30 (2014), 461–478.\n","[50] Jayakumar Subramanian, Raihan Seraj, and Aditya Mahajan. 2018. Reinforcement\n","learning for mean-field teams. In Workshop on Adaptive and Learning Agents at\n","the International Conference on Autonomous Agents and Multi-Agent Systems .\n","[51] Josh A Taylor, Sairaj V Dhople, and Duncan S Callaway. 2016. Power systems\n","without fuel. Renewable and Sustainable Energy Reviews 57 (2016), 1322–1336.\n","[52] Jose R. Vazquez-Canteli, Sourav Dey, Gregor Henze, and Zoltan Nagy. 2020.\n","CityLearn: Standardizing Research in Multi-Agent Reinforcement Learning for\n","Demand Response and Urban Energy Management. (Dec 2020). https://doi.org/\n","10.48550/arXiv.2012.10504 arXiv:2012.10504 [cs].\n","[53] Jose R. Vazquez-Canteli, Gregor Henze, and Zoltan Nagy. 2020. MARLISA:\n","Multi-Agent Reinforcement Learning with Iterative Sequential Action Selection\n","for Load Shaping of Grid-Interactive Connected Buildings. In Proceedings of\n","the 7th ACM International Conference on Systems for Energy-Efficient Buildings,\n","Cities, and Transportation (Virtual Event, Japan) (BuildSys ’20) . Association for\n","Computing Machinery, New York, NY, USA, 170–179. https://doi.org/10.1145/\n","3408308.3427604\n","[54] Zhe Wang, Bingqing Chen, Han Li, and Tianzhen Hong. 2021. AlphaBuilding\n","ResCommunity: A multi-agent virtual testbed for community-level load coordi-\n","nation. Advances in Applied Energy 4 (2021), 100061.[55] Hua Wei, Nan Xu, Huichu Zhang, Guanjie Zheng, Xinshi Zang, Chacha Chen,\n","Weinan Zhang, Yanmin Zhu, Kai Xu, and Zhenhui Li. 2019. CoLight: Learning\n","Network-level Cooperation for Traffic Signal Control. In Proceedings of the 28th\n","ACM International Conference on Information and Knowledge Management (CIKM\n","’19). Association for Computing Machinery, New York, NY, USA, 1913–1922.\n","https://doi.org/10.1145/3357384.3357902\n","[56] Xiaoyu Wu, Jinghan He, Yin Xu, Jian Lu, Ning Lu, and Xiaojun Wang. 2018.\n","Hierarchical Control of Residential HVAC Units for Primary Frequency Reg-\n","ulation. IEEE Transactions on Smart Grid 9, 4 (2018), 3844–3856. https:\n","//doi.org/10.1109/TSG.2017.2766880\n","[57] Lei Xi, Jianfeng Chen, Yuehua Huang, Yanchun Xu, Lang Liu, Yimin Zhou, and\n","Yudan Li. 2018. Smart generation control based on multi-agent reinforcement\n","learning with the idea of the time tunnel. Energy 153 (2018), 977–987. https:\n","//doi.org/10.1016/j.energy.2018.04.042\n","[58] Yaodong Yang, Jianye Hao, Yan Zheng, Xiaotian Hao, and Bofeng Fu. 2019. Large-\n","Scale Home Energy Management Using Entropy-Based Collective Multiagent\n","Reinforcement Learning Framework. In Proceedings of the 18th International\n","Conference on Autonomous Agents and MultiAgent Systems (AAMAS ’19) . Interna-\n","tional Foundation for Autonomous Agents and Multiagent Systems, Richland,\n","SC, 2285–2287.\n","[59] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang.\n","2018. Mean Field Multi-Agent Reinforcement Learning. (Feb 2018). https:\n","//doi.org/10.48550/arXiv.1802.05438\n","[60] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu.\n","2021. The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.\n","(Jul 2021). arXiv:2103.01955 [cs].\n","[61] Wei Zhang, Jianming Lian, Chin-Yao Chang, and Karanjit Kalsi. 2013. Aggregated\n","modeling and control of air conditioning loads for demand response. IEEE\n","transactions on power systems 28, 4 (2013), 4655–4664.\n","[62] X Zhou, E Dall’Anese, and L Chen. 2019. Online stochastic optimization of\n","networked distributed energy resources. IEEE Trans. on Automatic Control 65, 6\n","(2019), 2387–2401.A NOTATION\n","Table 6 contains the different notations we use in this paper.\n","B CARBON EMISSIONS OF THE RESEARCH\n","PROJECT\n","As a significant amount of electricity has been used to train and run\n","the models for this work, we publish its estimated carbon footprint.\n","Experiments were conducted using a private infrastructure, which\n","has a carbon efficiency of 0.049 kgCo 2eq/kWh. A cumulative of\n","10895 days, or 261480 hours, of computation was mainly performed\n","on CPU of type Intel Xeon Processor E5-2683 v4 (TDP of 120W).\n","We assume on average a power usage of half the TDP for CPUs.\n","The total emissions are estimated to be 628 kgCO 2eq of which\n","0% were directly offset. This is equivalent to 2550 km driven by an\n","average car, or 314 kg of burned coal.\n","These estimations were conducted using the MachineLearning\n","Impact calculator [30].\n","C ENVIRONMENT DETAILS\n","C.1 Detailed house thermal model\n","The air temperature in each house evolves separately, based on its\n","thermal characteristics 𝜃ℎ, its current state , the outdoor conditions\n","such as outdoor temperature and solar gain, and the status of the\n","air conditioner in the house. The second-order model is based on\n","Gridlab-D’s Residential module user’s guide [24].\n","Using Gridlab-D’s module, we model an 8 ×12.5 m, one level\n","rectangular house, with a ceiling height of 2.5 m, 4 1.8-m2, 2-layer,\n","aluminum windows, and 2 2-m2wooden doors, leading to the fol-\n","lowing values presented in Table 7.\n","To model the evolution of the house’s air temperature 𝑇ℎ,𝑡and\n","its mass temperature 𝑇𝑚,𝑡, we assume that this temperature is ho-\n","mogeneous and do not consider the heat propagation in the house.\n","We define the following variables:\n","𝑎=𝐶𝑚𝐶ℎ/𝐻𝑚\n","𝑏=𝐶𝑚(𝑈ℎ+𝐻𝑚)/𝐻𝑚+𝐶ℎ\n","𝑐=𝑈ℎ\n","𝑑=𝑄𝑎,𝑡+𝑄𝑠,𝑡+𝑈ℎ𝑇𝑜,𝑡\n","𝑑𝑇𝐴0/𝑑𝑇=\u0000𝐻𝑚𝑇𝑚,𝑡−(𝑈ℎ+𝐻𝑚)𝑇ℎ,𝑡\n","+𝑈ℎ𝑇𝑜,𝑡+𝑄ℎ,𝑡+𝑄𝑠,𝑡\u0001/𝐶ℎ.\n","The following coefficient are then computed:\n","𝑟1=(−𝑏+√︁\n","𝑏2−4𝑎𝑐)/2𝑎\n","𝑟2=(−𝑏−√︁\n","𝑏2−4𝑎𝑐)/2𝑎\n","𝐴1=(𝑟2𝑇𝑎,𝑡−𝑑𝑇𝐴0/𝑑𝑇−𝑟2𝑑/𝑐)/(𝑟2−𝑟1)\n","𝐴2=𝑇ℎ,𝑡−𝑑/𝑐−𝐴1\n","𝐴3=(𝑟1𝐶ℎ+𝑈ℎ+𝐻𝑚)/𝐻𝑚\n","𝐴4=(𝑟2𝐶ℎ+𝑈ℎ+𝐻𝑚)/𝐻𝑚.\n","These coefficients are finally applied to the following dynamic\n","equations:\n","𝑇𝑎,𝑡+1=𝐴1𝑒𝑟1𝛿𝑡+𝐴2𝑒𝑟2𝛿𝑡+𝑑/𝑐\n","𝑇𝑚,𝑡+1=𝐴1𝐴3𝑒𝑟𝑡𝛿𝑡+𝐴2𝐴4𝑒𝑟2𝛿𝑡+𝑑/𝑐.C.1.1 Solar gain. It is possible to add the solar gain to the simulator.\n","It is computed based on the CIBSE Environmental Design Guide\n","[15].\n","The house’s lighting characteristics 𝜃𝑆, which include the win-\n","dow area and the shading coefficient of 0.67 are needed to model\n","the solar gain, 𝑄𝑠,𝑡.\n","Then, the following assumptions are made:\n","•The latitude is 30◦.\n","•The solar gain is negligible before 7:30 am and after 5:30 pm\n","at such latitude.\n","•The windows are distributed evenly around the building, in\n","the 4 orientations.\n","•All windows are vertical.\n","This allows us to compute the coefficients of a fourth-degree\n","bivariate polynomial to model the solar gain of the house based on\n","the time of the day and the day of the year.\n","C.2 Detailed air conditioner model\n","Once again based on the Gridlab-D Residential module user’s guide\n","[24], we model the air conditioner’s power consumption 𝑃𝑎,𝑡when\n","turned on, and the heat retrieved from the air 𝑄𝑎,𝑡, based on its\n","characteristics 𝜃𝐻, such as cooling capacity 𝐾𝑎, coefficient of per-\n","formance𝐶𝑂𝑃𝑎, and the latent cooling fraction 𝐿𝑎.\n","𝐶𝑂𝑃𝑎and𝐿𝑎are considered constant and based on default values\n","of the guide: 𝐶𝑂𝑃𝑎=2.5and𝐿𝑎=0.35. We have:\n","𝑄𝑎,𝑡=−𝐾𝑎\n","1+𝐿𝑎\n","𝑃𝑎,𝑡=𝐾𝑎\n","𝐶𝑂𝑃𝑎.\n","We set𝐾𝑎to 15 kW, or 50 000 BTU/hr, to be able to control the air\n","temperature even with high outdoor temperatures. This is higher\n","than most house ACs, but allows to have sufficient flexibility even at\n","high outdoor temperatures (a 5kW AC would have to be always on\n","to keep a 20 °C temperature when it is 38 °C outside). This choice does\n","not significantly affect our results: with lower outdoor temperatures,\n","the problem is equivalent with lower AC power.\n","C.3 Regulation signal\n","C.3.1 Interpolation for the base signal. As described in Section\n","3.1.3, we estimate 𝐷𝑎,𝑡by interpolation. A bang-bang controller is\n","ran without lockout for 5 minutes, and we compute the average\n","power that was consumed. This gives a proxy for the amount of\n","power necessary in a given situation.\n","A database was created by estimating 𝐷𝑎,𝑡for a single house\n","for more than 4 million combinations of the following parameters:\n","the house thermal characteristics 𝜃ℎ, the differences between its\n","air and mass temperatures 𝑇𝑎,𝑡and𝑇𝑚,𝑡and the target temperature\n","𝑇𝑇, the outdoor temperature 𝑇𝑜,𝑡, and the AC’s cooling capacity 𝐾𝑎.\n","If the solar gain is added to the simulation, the hour of the day and\n","the day of the year are also considered.\n","When the environment is simulated, every 5 minutes, 𝐷𝑎,𝑡is\n","computed by summing the interpolated necessary consumption of\n","every house of the cluster. The interpolation process is linear for\n","most parameters except for the 4 elements of 𝜃ℎand for𝐾𝑎, whichTable 6: Notation table\n","Number of agents𝑁 Number of houses in cluster (general)\n","𝑁tr Number of houses in training environment\n","𝑁de Number of houses in test environment\n","𝑁ctr Number of agents for communication during training\n","𝑁cde Number of agents for communication at deployment\n","Temperatures𝑇ℎ Indoor air temperature\n","𝑇𝑚 Indoor mass temperature\n","𝑇𝑜 Outside temperature\n","𝑇𝑇 Target indoor temperature\n","Signal and power𝑠0 Base signal\n","𝜌 Power system operator signal\n","𝑠 Regulation signal\n","𝐷𝑎 Average power needed by the ACs\n","𝐷𝑜 Power needed by non flexible loads\n","𝛿𝑠 Signal variation\n","𝛿𝑝 Perlin noise\n","𝛽𝑝 Variation amplitude parameter\n","𝑃 Total cluster power consumption\n","AC state𝜔 Status ( onoroff)\n","𝑙 Time left for lockout\n","𝑃𝑎 Power consumption\n","𝑄𝑎 Heat removed by the AC\n","House thermal model𝜃ℎ House thermal characteristics\n","𝑈ℎ Outside walls conductance\n","𝐶𝑚 House thermal mass\n","𝐶ℎ Air thermal mass\n","𝐻𝑚 Mass surface conductance\n","𝜃𝑠 House lightning characteristics\n","𝑄𝑠 Solar gain\n","AC model𝜃𝑎 AC characteristics\n","𝐾𝑎 Cooling capacity\n","𝐶𝑂𝑃𝑎 Coefficient of performance\n","𝐿𝑎 Latent cooling fraction\n","𝑙max lockout duration\n","POMDP𝑎,A Action, action space\n","𝑜,O Observation, observation space\n","𝑆,S State, state space\n","𝑟,R Reward, reward function\n","P Transition probabilities\n","𝑀 Set of communicating agents\n","𝑚𝑖\n","𝑗Message from 𝑗to𝑖\n","˜𝑜 Concatenated observation and messages\n","𝛾 Discount factor\n","𝛼temp,𝛼sig Weights in the reward function\n","Algorithms𝐻 Horizon\n","Θ Transition\n","𝑄(𝑎,𝑜),𝑇(𝑎,𝑜)𝑄-value prediction (with Q or target network)\n","𝜋𝜃 Policy parameterized by 𝜃\n","𝑉𝜙 Critic parameterized by 𝜙\n","𝐺 Return\n","𝐴𝜋Advantage for policy 𝜋\n","are instead using nearest neighbours to reduce the complexity of\n","the operation.C.3.2 Perlin noise. 1-D Perlin noise is used to compute 𝛿Π,𝑡, the\n","power generation high-frequency element. Designed for the fieldTable 7: Default house thermal parameters 𝜃ℎ\n","𝑈ℎ2.18×102W/K\n","𝐶𝑚 3.45×106J/K\n","𝐶ℎ 9.08×105J/K\n","𝐻𝑚 2.84×103W/K\n","of computer image generation, this noise has several interesting\n","properties for our use case.\n","Perlin noise is most of the time generated by the superposition of\n","several sub-noises called octaves. It is possible to restrict the span of\n","the values that they can take. Thus, it is possible to test the agents\n","in an environment taking into account several frequencies of non-\n","regular noise, but whose values are restricted within realistic limits.\n","Moreover, the average value of the noise can be easily defined and\n","does not deviate, which ensures that for a sufficiently long time\n","horizon, the noise average is 0.\n","Each octave is characterized by 2 parameters: an amplitude and\n","a frequency ratio. The frequency represents the distance between\n","two random deviations. The amplitude represents the magnitude\n","of the variation. Normally the frequency increases as the amplitude\n","decreases. This way, high-amplitude noise is spread over a wider\n","interval and lower amplitude noise is more frequent and compact.\n","In our case, we use 5 octaves, with an amplitude ratio of 0.9\n","between each octave and a frequency proportional to the number\n","of the octave.\n","D ALGORITHM DETAILS\n","D.1 Model Predictive Control\n","Our MPC is based on a centralized model. At each time step, in-\n","formation about the state of the agents is used to find the future\n","controls that minimize the reward function over the next 𝐻time\n","steps. The optimal immediate action is then communicated to the\n","agents. At each time step, the algorithm calculates the ideal control\n","combination for the 𝐻-time step horizon.\n","The cost function for both the signal and the temperature to\n","minimize being the RMSE, the problem is modeled as a quadratic\n","mixed-integer program. The solver used to solve the MPC is the\n","commercial solver Gurobi [23] together with CVXPY [18]. Gurobi\n","being a licensed solver, its exact internal behavior is unknown\n","to us and it acts as a black box for our MPC. However, we know\n","that it solves convex integer problems using the branch and bound\n","algorithm. The speed of resolution depends mainly on the quality\n","of the solver’s heuristics.\n","The computation time required for each step of the MPC in-\n","creases drastically with the number of agents and/or 𝐻. To be able\n","to test this approach with enough agents and a rolling horizon al-\n","lowing to have reasonable performance, it was necessary to increase\n","the time step at which the agents make decisions to 12 seconds\n","(instead of 4 for other agents).\n","It was impossible to launch an experiment with the MPC agent\n","for 48 hours in a reasonable time. To compensate, we launched in\n","parallel 200 agents having been started at random simulated times.\n","In order to reach quickly the stability of the environment, the noiseon the temperature was reduced to 0.05 °C. We then measured the\n","average RMSE over the first 2 hours of simulation for each agent.\n","Despite this, it was impossible to test the MPC with more than\n","10 agents while keeping the computation time reasonable enough\n","to be used in real time. That is to say, in a time shorter than the\n","duration between two-time steps.\n","At each time step, the MPC solves the following optimization\n","problem :\n","min\n","𝑎∈{0,1}𝑁×𝐻∑︁\n","𝑡∈𝐻𝛼sig(∑︁\n","𝑖𝜖𝑁𝑃𝑖,𝑡−𝑠0)2+𝛼temp∑︁\n","𝑖𝜖𝑁(𝑇ℎ,𝑡,𝑖−𝑇𝑡,𝑡,𝑖)2,\n","such that it obeys the following physical constraints of the envi-\n","ronment:\n","𝑇ℎ,𝑡,𝑖,𝑇𝑚,𝑡,𝑖=𝐹1(𝑎𝑖,𝑡,𝑇ℎ,𝑡−1,𝑖,𝑇𝑚,𝑡−1,𝑖)∀𝑡∈𝐻,𝑖∈𝑁\n","𝑃ℎ,𝑡,𝑖=𝑎𝑖,𝑡𝐹2(𝜃𝑖\n","𝑎)∀𝑡∈𝐻,𝑖∈𝑁,\n","and the lockout constraint:\n","𝑙𝑚𝑎𝑥(𝑎𝑖,𝑡−𝜔𝑖,𝑡−1)−𝑙𝑚𝑎𝑥∑︁\n","𝑘=0(1−𝜔𝑖,𝑡−𝑘)≤0∀𝑡∈𝐻,𝑖∈𝑁,\n","where𝐹1and𝐹2are convex functions that can be deduced from the\n","physical equations given in Section C.\n","D.2 Learning-based methods\n","D.2.1 TarMAC and MA-PPO. The original implementation of Tar-\n","MAC [ 17] is built over the Asynchronous Advantage Actor-Critic\n","(A3C) algorithm [ 41]. The environments on which it is trained have\n","very short episodes, making it possible for the agents to train online\n","over the whole memory as one mini-batch.\n","This is not possible with our environment where training episodes\n","last around 16000 time steps. As a result, we built TarMAC over our\n","existing MA-PPO implementation. The same loss functions were\n","used to train the actor and the critic.\n","The critic is given all agents’ observation as an input.\n","The actor’s architecture is described in Figure 8. Agent 𝑖’s ob-\n","servations are passed through a first multi-layer perceptron (MLP),\n","outputting a hidden state 𝑥.𝑥is then used to produce a key, a\n","value, and a query by three MLPs. The key and value are sent to\n","the other agents, while agent 𝑖receives the other agents’ keys and\n","values. The other agents’ keys are multiplied using a dot product\n","with agent𝑖’s query, and passed through a softmax to produce the\n","attention. Here, a mask is applied to impose the localized communi-\n","cation constraints and ensure agent 𝑖only listen to its neighbours.\n","The attention is then used as weights for the values, which are\n","summed together to produce the communication vector for agent\n","𝑖. For multi-round communication, the communication vector and\n","𝑥are concatenated and passed through another MLP to produce a\n","new𝑥, and the communication process is repeated for the number\n","of communication hops. Once done, the final 𝑥and communication\n","vector are once more concatenated and passed through the last\n","MLP, the actor, to produce the action probabilities.\n","We take advantage of the centralized training approach to con-\n","nect the agents’ communications in the computational graph during\n","training. Once trained, the agents can be deployed in a decentralized\n","way.Figure 7: Illustration of how several octaves add up to form Perlin noise. The frequency of the octaves increases as their\n","amplitude decreases.\n","Figure 8: Architecture of the TarMAC-PPO actor\n","D.2.2 Neural networks architecture and optimization. For MA-DQN\n","as well as for MA-PPO-HE, every neural network has the same struc-\n","ture, except for the number of inputs and outputs. The networks\n","are composed of 2 hidden layers of 100 neurons, activated with\n","ReLU, and are trained with Adam [27].\n","For TarMAC-PPO, the actor’s obs2hidden ,hidden2key ,hidden2val ,\n","hidden2query andactor MLPs (as shown in Figure 8) all have onehidden layer of size 32. obs2hidden andactor are activated by ReLU\n","whereas the three communication MLPs are activated by hyperbolic\n","tangent. The hidden state 𝑥also has a size of 32.\n","The centralized critic is an MLP with two hidden layers of size\n","128 activated with ReLU. The input size is the number of agentsmultiplied by their observation size, and the output size is the\n","number of agents.\n","For all networks, the inputs are normalized by constants to facil-\n","itate the training. The networks are optimized using Adam.\n","D.2.3 Hyperparameters. We carefully tuned the hyperparameters\n","through grid searches. Table 8 shows the hyperparameters selected\n","for the agents presented in the paper.\n","Table 8: Training hyperparameters\n","Hyperparameter TarMAC-PPO MA-PPO DQN\n","Learning rate 0.001 0.001 0.0001\n","Mini-batch size 256 512 256\n","Clip parameter 0.2 0.2 -\n","Max grad norm 0.5 0.5 -\n","Number epochs 200 200 -\n","Number updates 10 10 -\n","Number episodes 200 200 -\n","Discount factor 𝛾 0.99 0.99 0.99\n","Key vector size 4 or 8 - -\n","Comm. vector size 8 - -\n","Number comm. rounds 1 - -\n","Buffer capacity - - 65536\n","𝜖decay - - 0.995\n","Min𝜖 - - 0.01E𝑁deAND PER-AGENT RMSE\n","In this section, we discuss the relation between the per-agent signal\n","RMSE of an aggregation of 𝑁homogeneous agents if 𝑁is multiplied\n","by an integer 𝑘∈N.\n","We consider the aggregation of size 𝑘𝑁as the aggregation of\n","𝑘homogeneous groups 𝑔𝑗of𝑁agents which consumes a power\n","𝑃𝑗\n","𝑔,𝑡=Í𝑁\n","𝑖𝑃𝑖\n","𝑎,𝑡. We have:𝑃𝑡=Í𝑘𝑁\n","𝑖𝑃𝑖\n","𝑎,𝑡=Í𝑘\n","𝑗𝑃𝑗\n","𝑔,𝑡.\n","We assume that each group tracks an equal portion of the signal\n","𝑠𝑗\n","𝑡=𝑠𝑡/𝑘. We assume that the tracking error 𝑃𝑗\n","𝑔,𝑡−𝑠𝑗\n","𝑡follows a\n","0-mean Gaussian of standard deviation 𝜎𝑔. This Gaussian error is\n","uncorrelated to the noise of other groups.\n","It follows from the properties of Gaussian random variables that\n","the aggregation signal error 𝑃𝑡−𝑠𝑡follows a Gaussian distribution\n","of mean𝜇𝑘=0and standard deviation 𝜎𝑘=√\n","𝑘𝜎𝑔for all𝑘≥1\n","with𝑘∈N.\n","Hence the signal’s RMSE of a group of 𝑘𝑁agents, which is a\n","measured estimation of 𝜎𝑘, is approximately√\n","𝑘times the RMSE\n","of a group of 𝑁agents, which estimates 𝜎𝑔. Finally, the per-agent\n","RMSE is computed as the group’s RMSE divided by the number of\n","agents. We therefore have that the per-agent RMSE of 𝑘𝑁agents is\n","approximately√\n","𝑘/𝑘=1/√\n","𝑘times the RMSE of 𝑁agents.\n","This discussion provides an intuitive explanation for the diminu-\n","tion of the relative RMSE when the number of agents increases.\n","However, it is based on the assumption that the error of each group\n","is not biased, which is not necessarily true with our agents. This ex-\n","plains why the RMSEs are not 10 times lower passing from 𝑁de=10\n","to𝑁de=1000.\n"]}],"source":["print(pdf_text)"]},{"cell_type":"markdown","id":"e517aa25","metadata":{"papermill":{},"tags":[],"id":"e517aa25"},"source":["### Loading the pipeline\n","Import Pipeline from Transformer after installing the transformers and tensorflow."]},{"cell_type":"code","execution_count":13,"id":"06c48acd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["c8d534d23d674d8e9c4caa888223e92d","e1c4bd83d4954d38ae297da528a2bafa","a212617126914d2ba981d6e7706085cc","92cb5a4a8ecb44b79167b4a44fff4310","58aa4a811f684c7eaa654354223cad43","8c225e8104c3448b9606e1a46aa3d1c6","2b38d682a0e0433fae7fdd3ceff9f5de","acd458e05017477daa2799359a305f3b","c185273e6079494eb9c1cde47874bc3b","39f6e6fd055b405385eefdab340b2403","0cc2c2ecee1e4f2eb957716c670a20d1","af98adcfa9da4f3698f13d3bd994adb7","4a26f6aaf138416d8554f8398e6f9582","49e3d278247147cbbed3ce6dbbc39555","67885baa70fa4a12964b4d2bce3cc30c","426b979938da4934af3334573a243e57","ce19041d113b4c77bec8d6bb6ff7708c","f860733f9f5d4ca3a288a6862a3e3a28","7f76ec56041c4e8ca570ae655dafbc50","39b8ef9c6fe74a458ea9ecbd6d5b6b9e","69a21645ae4445369e7ec76964a61c49","55960a333c624f6cac21be5b562fb161","8a33404b8d9f44aead76e8484cb9b2a1","fbfb31a26af74278b32703e9bdfeac17","d4861812e345450ab2df3a696ea69f80","f79e275f9a7b444ebc5cf3606dac99ed","4afdee6a37f14e96aae4b13b295cf65e","ca1469e4d4ba4085b7d7fbdf4f0842a3","a13807ada1c24a4bb18d7b63c05c1bb1","efcfdaf3a7b44b73a20c7a6e80750cc4","6a65637d65544a6ab85b60d926f0371a","7fe44539c259442395126dd6764e00aa","a72379c944c34fba9279cb62e16cea46","9239f01ec4a04dbba7f7fc1c28bfabbe","cab9b6984b0e44c0861802af59978fff","6f7dd4b7c6d4418ba6a88a7d4e1456a0","becb721b629742a0a26e4979d9fa3ccb","24c775df6f41434fbb58fcdb7c799dc8","b647f429cf5841f8b5dcd09a7bbe690a","f556fa0edb844ec9a43d71a67f62a559","0f968f0330bd40cab82ca8c84b28e5cd","a62b94f7d12c451cbc35813dd89ea8e9","645ea530f9da4fedbb39d69fd65ff342","90970ecda0f8413aacb1f6c6d1eac7e6","6131d594dc11450780687acd2fc0b2b8","3cbfdfcaab8c4747b21f4529c76bc7bc","e98ac486f3c4454a9d8c2311775f4fbe","d424e5ad10b94b9eb24ce314f9586f26","704742368ad24abda33b9080fd10780b","ce8efa025b404fe78456007fae41ed14","5d70accce1ce4c31ba4e5df3f3cb67ed","b9c22b508ec74e6c95211bbc5bfd0c00","6329d4b8479b45a8b7cb30d786350848","c9d11b74cf584570b89c3dd2e1d34a64","2740c5d726e44919a5b70c8155d30a77","25b01c1a89f84be4a1aa9144fe050b0f","da78b67f68834191b333b8d60cdcfdc8","85b4d29b53994175996201d4e7438c28","97092b0a03a44b058f0878c165fa498a","02a5e219659440aca1092a88e5282d2a","747aabb291584a01aa59c1bf7383276a","ae6e27d9924b4ab7ac568029291fb351","eeeccfbb2af54bd69dbbe334aa465952","f77e0f0a81cb4d5b95545d6e1c8b2454","bba165dfa7d7404ca4b9604d863dc334","6f2ede6069a245e5873b6ff806302fc6"]},"execution":{"iopub.execute_input":"2022-11-02T21:56:50.422719Z","iopub.status.busy":"2022-11-02T21:56:50.422379Z","iopub.status.idle":"2022-11-02T21:57:00.388711Z","shell.execute_reply":"2022-11-02T21:57:00.388119Z","shell.execute_reply.started":"2022-11-02T21:56:50.422690Z"},"id":"06c48acd","outputId":"e9e15cb5-601d-46d0-d2b9-1dbfa5af6161","papermill":{},"tags":[],"executionInfo":{"status":"ok","timestamp":1673638661822,"user_tz":0,"elapsed":12516,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8d534d23d674d8e9c4caa888223e92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af98adcfa9da4f3698f13d3bd994adb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a33404b8d9f44aead76e8484cb9b2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9239f01ec4a04dbba7f7fc1c28bfabbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6131d594dc11450780687acd2fc0b2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b01c1a89f84be4a1aa9144fe050b0f"}},"metadata":{}}],"source":["nlp = pipeline('question-answering', model='deepset/roberta-base-squad2', tokenizer='deepset/roberta-base-squad2')"]},{"cell_type":"markdown","id":"378de1e9","metadata":{"execution":{"iopub.execute_input":"2022-11-02T22:04:21.973105Z","iopub.status.busy":"2022-11-02T22:04:21.972816Z","iopub.status.idle":"2022-11-02T22:04:21.980111Z","shell.execute_reply":"2022-11-02T22:04:21.977454Z","shell.execute_reply.started":"2022-11-02T22:04:21.973076Z"},"papermill":{},"tags":[],"id":"378de1e9"},"source":["## Output"]},{"cell_type":"markdown","id":"fba0a8f9","metadata":{"papermill":{},"tags":[],"id":"fba0a8f9"},"source":["### Ask question"]},{"cell_type":"code","execution_count":null,"id":"acc20e20","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-11-02T21:59:11.948410Z","iopub.status.busy":"2022-11-02T21:59:11.948185Z","iopub.status.idle":"2022-11-02T21:59:40.862019Z","shell.execute_reply":"2022-11-02T21:59:40.861393Z","shell.execute_reply.started":"2022-11-02T21:59:11.948388Z"},"id":"acc20e20","outputId":"657a5421-55b5-4941-b127-929bb9b3173a","papermill":{},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your question:\n","What is the bang bang controller?\n"]}],"source":["context = pdf_text \n","question = input('Enter your question:\\n')\n","\n","question_set = {\n","        'context': context,\n","        'question': question\n","    }\n","\n","results = nlp(question_set)"]},{"cell_type":"markdown","id":"9f24f731","metadata":{"id":"9f24f731","papermill":{},"tags":[]},"source":["\n","\n","### Get answer\n","\n","This will print the answer to the question you have asked before."]},{"cell_type":"code","execution_count":15,"id":"45e3ccf1","metadata":{"execution":{"iopub.execute_input":"2022-11-02T22:00:18.496884Z","iopub.status.busy":"2022-11-02T22:00:18.496658Z","iopub.status.idle":"2022-11-02T22:00:18.502528Z","shell.execute_reply":"2022-11-02T22:00:18.501992Z","shell.execute_reply.started":"2022-11-02T22:00:18.496863Z"},"papermill":{},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"45e3ccf1","executionInfo":{"status":"ok","timestamp":1673638951390,"user_tz":0,"elapsed":253,"user":{"displayName":"Mohamed Numair","userId":"01893332458841847125"}},"outputId":"d75793d1-66f9-4529-bb38-91b75faabc8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Answer: residential air condition-\n","ers\n"]}],"source":["print(\"\\nAnswer: \" + results['answer'])"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"papermill":{"default_parameters":{},"environment_variables":{},"parameters":{},"version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c8d534d23d674d8e9c4caa888223e92d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1c4bd83d4954d38ae297da528a2bafa","IPY_MODEL_a212617126914d2ba981d6e7706085cc","IPY_MODEL_92cb5a4a8ecb44b79167b4a44fff4310"],"layout":"IPY_MODEL_58aa4a811f684c7eaa654354223cad43"}},"e1c4bd83d4954d38ae297da528a2bafa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c225e8104c3448b9606e1a46aa3d1c6","placeholder":"​","style":"IPY_MODEL_2b38d682a0e0433fae7fdd3ceff9f5de","value":"Downloading: 100%"}},"a212617126914d2ba981d6e7706085cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acd458e05017477daa2799359a305f3b","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c185273e6079494eb9c1cde47874bc3b","value":571}},"92cb5a4a8ecb44b79167b4a44fff4310":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39f6e6fd055b405385eefdab340b2403","placeholder":"​","style":"IPY_MODEL_0cc2c2ecee1e4f2eb957716c670a20d1","value":" 571/571 [00:00&lt;00:00, 26.4kB/s]"}},"58aa4a811f684c7eaa654354223cad43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c225e8104c3448b9606e1a46aa3d1c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b38d682a0e0433fae7fdd3ceff9f5de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acd458e05017477daa2799359a305f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c185273e6079494eb9c1cde47874bc3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39f6e6fd055b405385eefdab340b2403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cc2c2ecee1e4f2eb957716c670a20d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af98adcfa9da4f3698f13d3bd994adb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a26f6aaf138416d8554f8398e6f9582","IPY_MODEL_49e3d278247147cbbed3ce6dbbc39555","IPY_MODEL_67885baa70fa4a12964b4d2bce3cc30c"],"layout":"IPY_MODEL_426b979938da4934af3334573a243e57"}},"4a26f6aaf138416d8554f8398e6f9582":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce19041d113b4c77bec8d6bb6ff7708c","placeholder":"​","style":"IPY_MODEL_f860733f9f5d4ca3a288a6862a3e3a28","value":"Downloading: 100%"}},"49e3d278247147cbbed3ce6dbbc39555":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f76ec56041c4e8ca570ae655dafbc50","max":496313727,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39b8ef9c6fe74a458ea9ecbd6d5b6b9e","value":496313727}},"67885baa70fa4a12964b4d2bce3cc30c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a21645ae4445369e7ec76964a61c49","placeholder":"​","style":"IPY_MODEL_55960a333c624f6cac21be5b562fb161","value":" 496M/496M [00:08&lt;00:00, 61.6MB/s]"}},"426b979938da4934af3334573a243e57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce19041d113b4c77bec8d6bb6ff7708c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f860733f9f5d4ca3a288a6862a3e3a28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f76ec56041c4e8ca570ae655dafbc50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39b8ef9c6fe74a458ea9ecbd6d5b6b9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69a21645ae4445369e7ec76964a61c49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55960a333c624f6cac21be5b562fb161":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a33404b8d9f44aead76e8484cb9b2a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbfb31a26af74278b32703e9bdfeac17","IPY_MODEL_d4861812e345450ab2df3a696ea69f80","IPY_MODEL_f79e275f9a7b444ebc5cf3606dac99ed"],"layout":"IPY_MODEL_4afdee6a37f14e96aae4b13b295cf65e"}},"fbfb31a26af74278b32703e9bdfeac17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca1469e4d4ba4085b7d7fbdf4f0842a3","placeholder":"​","style":"IPY_MODEL_a13807ada1c24a4bb18d7b63c05c1bb1","value":"Downloading: 100%"}},"d4861812e345450ab2df3a696ea69f80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efcfdaf3a7b44b73a20c7a6e80750cc4","max":79,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a65637d65544a6ab85b60d926f0371a","value":79}},"f79e275f9a7b444ebc5cf3606dac99ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fe44539c259442395126dd6764e00aa","placeholder":"​","style":"IPY_MODEL_a72379c944c34fba9279cb62e16cea46","value":" 79.0/79.0 [00:00&lt;00:00, 4.51kB/s]"}},"4afdee6a37f14e96aae4b13b295cf65e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca1469e4d4ba4085b7d7fbdf4f0842a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a13807ada1c24a4bb18d7b63c05c1bb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efcfdaf3a7b44b73a20c7a6e80750cc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a65637d65544a6ab85b60d926f0371a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fe44539c259442395126dd6764e00aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a72379c944c34fba9279cb62e16cea46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9239f01ec4a04dbba7f7fc1c28bfabbe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cab9b6984b0e44c0861802af59978fff","IPY_MODEL_6f7dd4b7c6d4418ba6a88a7d4e1456a0","IPY_MODEL_becb721b629742a0a26e4979d9fa3ccb"],"layout":"IPY_MODEL_24c775df6f41434fbb58fcdb7c799dc8"}},"cab9b6984b0e44c0861802af59978fff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b647f429cf5841f8b5dcd09a7bbe690a","placeholder":"​","style":"IPY_MODEL_f556fa0edb844ec9a43d71a67f62a559","value":"Downloading: 100%"}},"6f7dd4b7c6d4418ba6a88a7d4e1456a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f968f0330bd40cab82ca8c84b28e5cd","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a62b94f7d12c451cbc35813dd89ea8e9","value":898822}},"becb721b629742a0a26e4979d9fa3ccb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_645ea530f9da4fedbb39d69fd65ff342","placeholder":"​","style":"IPY_MODEL_90970ecda0f8413aacb1f6c6d1eac7e6","value":" 899k/899k [00:00&lt;00:00, 2.08MB/s]"}},"24c775df6f41434fbb58fcdb7c799dc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b647f429cf5841f8b5dcd09a7bbe690a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f556fa0edb844ec9a43d71a67f62a559":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f968f0330bd40cab82ca8c84b28e5cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a62b94f7d12c451cbc35813dd89ea8e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"645ea530f9da4fedbb39d69fd65ff342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90970ecda0f8413aacb1f6c6d1eac7e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6131d594dc11450780687acd2fc0b2b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cbfdfcaab8c4747b21f4529c76bc7bc","IPY_MODEL_e98ac486f3c4454a9d8c2311775f4fbe","IPY_MODEL_d424e5ad10b94b9eb24ce314f9586f26"],"layout":"IPY_MODEL_704742368ad24abda33b9080fd10780b"}},"3cbfdfcaab8c4747b21f4529c76bc7bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce8efa025b404fe78456007fae41ed14","placeholder":"​","style":"IPY_MODEL_5d70accce1ce4c31ba4e5df3f3cb67ed","value":"Downloading: 100%"}},"e98ac486f3c4454a9d8c2311775f4fbe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9c22b508ec74e6c95211bbc5bfd0c00","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6329d4b8479b45a8b7cb30d786350848","value":456318}},"d424e5ad10b94b9eb24ce314f9586f26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9d11b74cf584570b89c3dd2e1d34a64","placeholder":"​","style":"IPY_MODEL_2740c5d726e44919a5b70c8155d30a77","value":" 456k/456k [00:00&lt;00:00, 3.38MB/s]"}},"704742368ad24abda33b9080fd10780b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce8efa025b404fe78456007fae41ed14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d70accce1ce4c31ba4e5df3f3cb67ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9c22b508ec74e6c95211bbc5bfd0c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6329d4b8479b45a8b7cb30d786350848":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9d11b74cf584570b89c3dd2e1d34a64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2740c5d726e44919a5b70c8155d30a77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25b01c1a89f84be4a1aa9144fe050b0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da78b67f68834191b333b8d60cdcfdc8","IPY_MODEL_85b4d29b53994175996201d4e7438c28","IPY_MODEL_97092b0a03a44b058f0878c165fa498a"],"layout":"IPY_MODEL_02a5e219659440aca1092a88e5282d2a"}},"da78b67f68834191b333b8d60cdcfdc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_747aabb291584a01aa59c1bf7383276a","placeholder":"​","style":"IPY_MODEL_ae6e27d9924b4ab7ac568029291fb351","value":"Downloading: 100%"}},"85b4d29b53994175996201d4e7438c28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeeccfbb2af54bd69dbbe334aa465952","max":772,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f77e0f0a81cb4d5b95545d6e1c8b2454","value":772}},"97092b0a03a44b058f0878c165fa498a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bba165dfa7d7404ca4b9604d863dc334","placeholder":"​","style":"IPY_MODEL_6f2ede6069a245e5873b6ff806302fc6","value":" 772/772 [00:00&lt;00:00, 20.5kB/s]"}},"02a5e219659440aca1092a88e5282d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"747aabb291584a01aa59c1bf7383276a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae6e27d9924b4ab7ac568029291fb351":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeeccfbb2af54bd69dbbe334aa465952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f77e0f0a81cb4d5b95545d6e1c8b2454":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bba165dfa7d7404ca4b9604d863dc334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f2ede6069a245e5873b6ff806302fc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
